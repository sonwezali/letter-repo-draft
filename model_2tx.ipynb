{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb0769e4-65bf-4232-9834-a6a2d523107f",
   "metadata": {},
   "source": [
    "# Create the size estimation csv if not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536e720b-c6b0-4b9d-a6b1-e1550528cd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_index</th>\n",
       "      <th>tx_index</th>\n",
       "      <th>method</th>\n",
       "      <th>est_size</th>\n",
       "      <th>est_center</th>\n",
       "      <th>true_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Density-weighted centers (on KMeans partitions)</td>\n",
       "      <td>3489</td>\n",
       "      <td>[3.675564212004624, 1.0209360816840582, -1.653...</td>\n",
       "      <td>3701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GMM means</td>\n",
       "      <td>2201</td>\n",
       "      <td>[3.9690072500659626, 1.1814134845627842, -1.75...</td>\n",
       "      <td>3701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>KMeans centers</td>\n",
       "      <td>3489</td>\n",
       "      <td>[3.2141649909603878, 0.834196541087591, -1.757...</td>\n",
       "      <td>3701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MCD-inlier density-weighted centers (on KMeans...</td>\n",
       "      <td>3489</td>\n",
       "      <td>[3.964486847638297, 1.2195497596063765, -1.593...</td>\n",
       "      <td>3701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mean Shift centers (MeanShift bandwidth=1.79, ...</td>\n",
       "      <td>2854</td>\n",
       "      <td>[4.469802735985509, 0.997070926347141, -1.5710...</td>\n",
       "      <td>3701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>9999</td>\n",
       "      <td>1</td>\n",
       "      <td>GMM means</td>\n",
       "      <td>3784</td>\n",
       "      <td>[-0.005330030300972829, 0.598339170900159, -1....</td>\n",
       "      <td>1609.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>9999</td>\n",
       "      <td>1</td>\n",
       "      <td>KMeans centers</td>\n",
       "      <td>1805</td>\n",
       "      <td>[2.765690554348608, 1.3953316396413447, -0.445...</td>\n",
       "      <td>1609.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>9999</td>\n",
       "      <td>1</td>\n",
       "      <td>MCD-inlier density-weighted centers (on KMeans...</td>\n",
       "      <td>1805</td>\n",
       "      <td>[3.2817243241126914, 1.8729069963852762, -1.08...</td>\n",
       "      <td>1609.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>9999</td>\n",
       "      <td>1</td>\n",
       "      <td>Mean Shift centers (MeanShift bandwidth=1.6, k...</td>\n",
       "      <td>395</td>\n",
       "      <td>[1.888018349244157, 2.8038930841001792, -3.503...</td>\n",
       "      <td>1609.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>9999</td>\n",
       "      <td>1</td>\n",
       "      <td>Robust (MinCovDet) centers (on KMeans partitions)</td>\n",
       "      <td>1805</td>\n",
       "      <td>[3.2757498561118403, 1.8003836699854403, -1.18...</td>\n",
       "      <td>1609.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        file_index  tx_index  \\\n",
       "0                0         0   \n",
       "1                0         0   \n",
       "2                0         0   \n",
       "3                0         0   \n",
       "4                0         0   \n",
       "...            ...       ...   \n",
       "119995        9999         1   \n",
       "119996        9999         1   \n",
       "119997        9999         1   \n",
       "119998        9999         1   \n",
       "119999        9999         1   \n",
       "\n",
       "                                                   method  est_size  \\\n",
       "0         Density-weighted centers (on KMeans partitions)      3489   \n",
       "1                                               GMM means      2201   \n",
       "2                                          KMeans centers      3489   \n",
       "3       MCD-inlier density-weighted centers (on KMeans...      3489   \n",
       "4       Mean Shift centers (MeanShift bandwidth=1.79, ...      2854   \n",
       "...                                                   ...       ...   \n",
       "119995                                          GMM means      3784   \n",
       "119996                                     KMeans centers      1805   \n",
       "119997  MCD-inlier density-weighted centers (on KMeans...      1805   \n",
       "119998  Mean Shift centers (MeanShift bandwidth=1.6, k...       395   \n",
       "119999  Robust (MinCovDet) centers (on KMeans partitions)      1805   \n",
       "\n",
       "                                               est_center  true_size  \n",
       "0       [3.675564212004624, 1.0209360816840582, -1.653...     3701.0  \n",
       "1       [3.9690072500659626, 1.1814134845627842, -1.75...     3701.0  \n",
       "2       [3.2141649909603878, 0.834196541087591, -1.757...     3701.0  \n",
       "3       [3.964486847638297, 1.2195497596063765, -1.593...     3701.0  \n",
       "4       [4.469802735985509, 0.997070926347141, -1.5710...     3701.0  \n",
       "...                                                   ...        ...  \n",
       "119995  [-0.005330030300972829, 0.598339170900159, -1....     1609.0  \n",
       "119996  [2.765690554348608, 1.3953316396413447, -0.445...     1609.0  \n",
       "119997  [3.2817243241126914, 1.8729069963852762, -1.08...     1609.0  \n",
       "119998  [1.888018349244157, 2.8038930841001792, -3.503...     1609.0  \n",
       "119999  [3.2757498561118403, 1.8003836699854403, -1.18...     1609.0  \n",
       "\n",
       "[120000 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def to_list(s):\n",
    "    \"\"\"Parse Python-literal lists stored as strings; pass through lists unchanged.\"\"\"\n",
    "    if isinstance(s, list):\n",
    "        return s\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    return ast.literal_eval(str(s))\n",
    "\n",
    "def build_long_rows(row):\n",
    "    \"\"\"\n",
    "    From one wide row produce per-cluster rows:\n",
    "      file_index  -> number from 'file' (e.g., '0.csv' -> 0)\n",
    "      tx_index    -> matched *true* index (c)\n",
    "      method      -> passthrough\n",
    "      est_size    -> cluster_sizes[r]\n",
    "      est_center  -> centers_est[r]\n",
    "      true_size   -> true_cluster_sizes[c]  (or true_cluster_sizes_matched[r], equivalent)\n",
    "    \"\"\"\n",
    "    file_name = str(row[\"file\"])\n",
    "    # extract number before \".csv\", fallback to original if not numeric\n",
    "    base = os.path.splitext(os.path.basename(file_name))[0]\n",
    "    try:\n",
    "        file_index = int(base)\n",
    "    except ValueError:\n",
    "        file_index = base\n",
    "\n",
    "    method = row[\"method\"]\n",
    "\n",
    "    # parse arrays\n",
    "    cluster_sizes                = to_list(row.get(\"cluster_sizes\", []))\n",
    "    true_cluster_sizes           = to_list(row.get(\"true_cluster_sizes\", []))\n",
    "    true_cluster_sizes_matched   = to_list(row.get(\"true_cluster_sizes_matched\", []))\n",
    "    centers_est                  = to_list(row.get(\"centers_est\", []))\n",
    "    match_row_ind               = to_list(row.get(\"match_row_ind\", []))\n",
    "    match_col_ind               = to_list(row.get(\"match_col_ind\", []))\n",
    "\n",
    "    # prefer explicit pairing (r -> c) via match indices\n",
    "    use_matched_vector = len(true_cluster_sizes_matched) == len(cluster_sizes) and len(cluster_sizes) > 0\n",
    "\n",
    "    out = []\n",
    "    # If we have Hungarian pairing, zip (r, c). Otherwise, assume identity mapping.\n",
    "    pairs = list(zip(match_row_ind, match_col_ind)) if match_row_ind and match_col_ind else [(i, i) for i in range(len(cluster_sizes))]\n",
    "\n",
    "    for r, c in pairs:\n",
    "        # guard against out-of-range indices\n",
    "        if r is None or c is None: \n",
    "            continue\n",
    "        if r < 0 or r >= len(cluster_sizes): \n",
    "            continue\n",
    "        # est fields\n",
    "        est_size   = int(cluster_sizes[r]) if pd.notna(cluster_sizes[r]) else None\n",
    "        est_center = centers_est[r] if r < len(centers_est) else None\n",
    "\n",
    "        # true size: either via matched array at r, or via c into true_cluster_sizes\n",
    "        if use_matched_vector:\n",
    "            true_size = int(true_cluster_sizes_matched[r])\n",
    "        else:\n",
    "            if c < 0 or c >= len(true_cluster_sizes):\n",
    "                true_size = None\n",
    "            else:\n",
    "                true_size = int(true_cluster_sizes[c])\n",
    "\n",
    "        # tx_index is the *true* index (c) that r matched to\n",
    "        tx_index = int(c)\n",
    "\n",
    "        out.append({\n",
    "            \"file_index\": file_index,\n",
    "            \"tx_index\": tx_index,\n",
    "            \"method\": method,\n",
    "            \"est_size\": est_size,\n",
    "            \"est_center\": est_center,\n",
    "            \"true_size\": true_size,\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def convert_wide_to_long(in_csv: str, out_csv: str):\n",
    "    df = pd.read_csv(in_csv)\n",
    "\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        rows.extend(build_long_rows(r))\n",
    "    out = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\"file_index\",\"tx_index\",\"method\",\"est_size\",\"est_center\",\"true_size\"]\n",
    "    )\n",
    "\n",
    "    # --- remove duplicates where file_index, tx_index, method are the same ---\n",
    "    before = len(out)\n",
    "    out = out.drop_duplicates(subset=[\"file_index\",\"tx_index\",\"method\"], keep=\"first\")\n",
    "    after = len(out)\n",
    "    if after < before:\n",
    "        print(f\"[info] dropped {before-after} duplicate rows with same (file_index, tx_index, method)\")\n",
    "\n",
    "    # --- fill NaN true_size from KMeans centers per (file_index, tx_index) ---\n",
    "    # Make sure true_size is numeric so NaNs are recognized\n",
    "    out[\"true_size\"] = pd.to_numeric(out[\"true_size\"], errors=\"coerce\")\n",
    "\n",
    "    # Lookup table: KMeans centers -> true_size_km\n",
    "    km = (\n",
    "        out[out[\"method\"] == \"KMeans centers\"]\n",
    "        .loc[:, [\"file_index\", \"tx_index\", \"true_size\"]]\n",
    "        .rename(columns={\"true_size\": \"true_size_km\"})\n",
    "    )\n",
    "\n",
    "    # Merge and fill\n",
    "    out = out.merge(km, on=[\"file_index\", \"tx_index\"], how=\"left\")\n",
    "    # Only fill where current true_size is NaN; leave KMeans row itself unchanged even if NaN\n",
    "    mask_needs_fill = out[\"true_size\"].isna() & out[\"true_size_km\"].notna()\n",
    "    out.loc[mask_needs_fill, \"true_size\"] = out.loc[mask_needs_fill, \"true_size_km\"]\n",
    "    out = out.drop(columns=[\"true_size_km\"])\n",
    "\n",
    "    out.sort_values(by=[\"file_index\",\"tx_index\",\"method\"], inplace=True, ignore_index=True)\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    return out\n",
    "\n",
    "convert_wide_to_long(\"./dataset/angle_results/all_results_enhanced.csv\", \"size_estimation_2Tx.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada79991-ff63-4f89-88b6-d28c36134ecd",
   "metadata": {},
   "source": [
    "# Angle Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cbfa9f-994c-4ac1-bfef-162984051229",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'uniform_test/config.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mtx_index\u001b[39m\u001b[33m\"\u001b[39m]   = df[\u001b[33m\"\u001b[39m\u001b[33mtx_index\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Load config truth\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m cfg = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muniform_test/config.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mfile_index\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cfg.columns:\n\u001b[32m     52\u001b[39m     cfg = cfg.reset_index().rename(columns={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mfile_index\u001b[39m\u001b[33m\"\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib64/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib64/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib64/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib64/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib64/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'uniform_test/config.csv'"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============================================================\n",
    "# Repro\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "\n",
    "def to_length(v, L=5.0, eps=1e-12):\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    n = np.linalg.norm(v)\n",
    "    if n < eps:\n",
    "        return v.tolist()  # leave zero vector as-is\n",
    "    return (v * (L / n)).tolist()\n",
    "\n",
    "def to_vec3(x):\n",
    "    if isinstance(x, list): return [float(x[0]), float(x[1]), float(x[2])]\n",
    "    if isinstance(x, str):  return list(map(float, ast.literal_eval(x)))\n",
    "    raise ValueError(\"Unexpected type for 3D vector\")\n",
    "\n",
    "def angle_error_deg(pred, true):\n",
    "    \"\"\"Mean angle error in degrees between two sets of 3D vectors\"\"\"\n",
    "    pred = np.asarray(pred, dtype=float)\n",
    "    true = np.asarray(true, dtype=float)\n",
    "    pred /= np.linalg.norm(pred, axis=1, keepdims=True) + 1e-12\n",
    "    true /= np.linalg.norm(true, axis=1, keepdims=True) + 1e-12\n",
    "    cos = np.clip(np.sum(pred*true, axis=1), -1.0, 1.0)\n",
    "    return np.degrees(np.arccos(cos))\n",
    "\n",
    "# ============================================================\n",
    "# Load estimated data\n",
    "df = pd.read_csv(\"size_estimation_2Tx.csv\")\n",
    "df = df[df[\"method\"] == \"KMeans centers\"].copy()\n",
    "# df = df[df[\"method\"] == \"MCD-inlier density-weighted centers (on KMeans partitions)\"].copy()\n",
    "\n",
    "\n",
    "df[\"est_center\"] = df[\"est_center\"].apply(to_vec3)\n",
    "df[\"est_size\"]   = df[\"est_size\"].astype(float)\n",
    "df[\"tx_index\"]   = df[\"tx_index\"].astype(int)\n",
    "\n",
    "# Load config truth\n",
    "cfg = pd.read_csv(\"dataset/config.csv\")\n",
    "if \"file_index\" not in cfg.columns:\n",
    "    cfg = cfg.reset_index().rename(columns={\"index\":\"file_index\"})\n",
    "\n",
    "truth_map = {}\n",
    "for _, r in cfg.iterrows():\n",
    "    centers = r[\"tx_centers\"]\n",
    "    centers = ast.literal_eval(centers) if isinstance(centers, str) else centers\n",
    "    centers = [list(map(float, c)) for c in centers]\n",
    "    # scale each ground-truth center to have length 5.0\n",
    "    centers = [to_length(c, 5.0) for c in centers]\n",
    "    truth_map[int(r[\"file_index\"])] = centers\n",
    "\n",
    "# Keep only scenes present in truth\n",
    "df = df[df[\"file_index\"].isin(truth_map.keys())]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Infer K from ground-truth config and build generalized dataset\n",
    "# ------------------------------------------------------------\n",
    "K = None\n",
    "for fid, centers in truth_map.items():\n",
    "    if K is None:\n",
    "        K = len(centers)\n",
    "    else:\n",
    "        assert K == len(centers), f\"Inconsistent K across files (file {fid}).\"\n",
    "print(f\"Detected K = {K} clusters per scene.\")\n",
    "\n",
    "# Keep only scenes that have all K tx_index present (for the chosen method)\n",
    "def has_all_k(s):\n",
    "    return set(s[\"tx_index\"].unique()) == set(range(K))\n",
    "\n",
    "g = (df.groupby(\"file_index\", as_index=False)\n",
    "       .filter(has_all_k)\n",
    "       .sort_values([\"file_index\", \"tx_index\"]))\n",
    "\n",
    "# Build rows with K clusters per scene\n",
    "rows = []\n",
    "for fid, s in g.groupby(\"file_index\"):\n",
    "    s = s.sort_values(\"tx_index\")\n",
    "    if list(s[\"tx_index\"]) != list(range(K)):\n",
    "        continue  # safety\n",
    "\n",
    "    # Estimated features: [size_k, est_center_k (3)] for k in 0..K-1\n",
    "    feat = []\n",
    "    for k in range(K):\n",
    "        feat.append(float(s.loc[s[\"tx_index\"] == k, \"est_size\"].values[0]))\n",
    "        feat.extend(list(map(float, s.loc[s[\"tx_index\"] == k, \"est_center\"].values[0])))\n",
    "\n",
    "    # True centers (each already scaled to length 5.0 above)\n",
    "    tcenters = truth_map[int(fid)]\n",
    "    target = []\n",
    "    for k in range(K):\n",
    "        target.extend(list(map(float, tcenters[k])))\n",
    "\n",
    "    rows.append({\"file_index\": int(fid), \"X\": feat, \"y\": target})\n",
    "\n",
    "pairs = pd.DataFrame(rows)\n",
    "print(f\"Built {len(pairs)} training rows with K={K}.\")\n",
    "\n",
    "# ============================================================\n",
    "# Features (input) & Targets (true centers, 3K)\n",
    "X = np.array(pairs[\"X\"].to_list(), dtype=float)      # (N, 4*K)\n",
    "y = np.array(pairs[\"y\"].to_list(), dtype=float)      # (N, 3*K)\n",
    "\n",
    "# ============================================================\n",
    "indices = np.arange(len(pairs))\n",
    "\n",
    "# First split: 70% train, 30% temp (for val + test)\n",
    "X_train, X_temp, y_train, y_temp, idx_train, idx_temp = train_test_split(\n",
    "    X, y, indices, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 50% of temp -> 15% val, 15% test\n",
    "X_val, X_test, y_val, y_test, idx_val, idx_test = train_test_split(\n",
    "    X_temp, y_temp, idx_temp, test_size=0.50, random_state=42\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Save train/val/test split into CSV\n",
    "# ============================================================\n",
    "train_idx = pairs.iloc[idx_train][\"file_index\"].reset_index(drop=True)\n",
    "val_idx   = pairs.iloc[idx_val][\"file_index\"].reset_index(drop=True)\n",
    "test_idx  = pairs.iloc[idx_test][\"file_index\"].reset_index(drop=True)\n",
    "\n",
    "split_df = pd.DataFrame({\n",
    "    \"file_index\": pd.concat([train_idx, val_idx, test_idx], ignore_index=True),\n",
    "    \"split\":     [\"train\"] * len(train_idx) + \\\n",
    "                 [\"validation\"] * len(val_idx) + \\\n",
    "                 [\"test\"] * len(test_idx)\n",
    "})\n",
    "\n",
    "split_df.to_csv(\"train_val_test_split_2Tx.csv\", index=False)\n",
    "print(\"Saved train/val/test split info to train_val_test_split_2Tx.csv\")\n",
    "\n",
    "# Scale inputs & outputs\n",
    "scaler_X = StandardScaler()\n",
    "X_train_s = scaler_X.fit_transform(X_train)\n",
    "X_val_s   = scaler_X.transform(X_val) \n",
    "X_test_s  = scaler_X.transform(X_test)\n",
    "\n",
    "y_mean = y_train.mean(axis=0)\n",
    "y_std  = y_train.std(axis=0); y_std[y_std==0] = 1.0\n",
    "y_train_s = (y_train - y_mean) / y_std\n",
    "y_val_s   = (y_val - y_mean) / y_std \n",
    "y_test_s  = (y_test - y_mean) / y_std\n",
    "\n",
    "Xtr = torch.tensor(X_train_s, dtype=torch.float32)\n",
    "ytr = torch.tensor(y_train_s, dtype=torch.float32)\n",
    "Xva = torch.tensor(X_val_s,   dtype=torch.float32) \n",
    "yva = torch.tensor(y_val_s,   dtype=torch.float32) \n",
    "Xte = torch.tensor(X_test_s,  dtype=torch.float32)\n",
    "yte = torch.tensor(y_test_s,  dtype=torch.float32)\n",
    "\n",
    "# ============================================================\n",
    "# Random-permutation augmentation helper (generalizes random swap)\n",
    "def random_permute_batch(X, y, K, p=0.5):\n",
    "    \"\"\"\n",
    "    X: (B, 4*K) -> blocks of 4 per cluster [size, cx, cy, cz]\n",
    "    y: (B, 3*K) -> blocks of 3 per cluster [tx, ty, tz]\n",
    "    With probability p per sample, apply a random permutation of the K clusters\n",
    "    to both X and y consistently.\n",
    "    \"\"\"\n",
    "    B = X.size(0)\n",
    "    device = X.device\n",
    "\n",
    "    Xb = X.view(B, K, 4).clone()\n",
    "    yb = y.view(B, K, 3).clone()\n",
    "\n",
    "    mask = (torch.rand(B, device=device) < p)\n",
    "    idxs = torch.arange(B, device=device)[mask]\n",
    "\n",
    "    for i in idxs.tolist():\n",
    "        perm = torch.randperm(K, device=device)\n",
    "        Xb[i] = Xb[i][perm]\n",
    "        yb[i] = yb[i][perm]\n",
    "\n",
    "    return Xb.view(B, 4*K), yb.view(B, 3*K)\n",
    "\n",
    "# ============================================================\n",
    "# Residual MLP (dynamic I/O sizes)\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, p_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "    def forward(self, x):\n",
    "        h = self.act(self.fc1(x))\n",
    "        h = self.drop(self.fc2(h))\n",
    "        return x + h\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim=256, depth=6, p_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(nn.Linear(in_dim, hidden_dim), nn.ReLU())\n",
    "        self.blocks = nn.Sequential(*[ResidualBlock(hidden_dim, p_drop) for _ in range(depth)])\n",
    "        self.head = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.blocks(x)\n",
    "        return self.head(x)\n",
    "\n",
    "in_dim = 4 * K   # per cluster: [size, cx, cy, cz]\n",
    "out_dim = 3 * K  # per cluster: [tx, ty, tz]\n",
    "\n",
    "model = ResidualMLP(in_dim=in_dim, out_dim=out_dim)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# ============================================================\n",
    "# Training loop (uses random permutation augmentation)\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "EPOCHS = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    # --- random permutation augmentation applied each epoch ---\n",
    "    Xb, yb = random_permute_batch(Xtr, ytr, K=K, p=0.5)\n",
    "\n",
    "    pred = model(Xb)\n",
    "    loss = loss_fn(pred, yb)\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # --- Use validation set for early stopping ---\n",
    "        val_loss = loss_fn(model(Xva), yva).item()\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Train {loss.item():.6f} | Val {val_loss:.6f}\")\n",
    "\n",
    "# Restore best model based on validation performance\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# ============================================================\n",
    "# Predict & unscale\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_s = model(Xte).cpu().numpy()\n",
    "\n",
    "y_pred = y_pred_s * y_std + y_mean   # (N, 3*K)\n",
    "\n",
    "# ============================================================\n",
    "# Generalized Angle error evaluation (for arbitrary K)\n",
    "# Extract estimated centers from raw X_test (unscaled), blocks of 4: [size, cx, cy, cz]\n",
    "est_blocks = [X_test[:, 1 + 4*k : 1 + 4*k + 3] for k in range(K)]  # each (N,3)\n",
    "\n",
    "errs_raw = []\n",
    "errs_ann = []\n",
    "for k in range(K):\n",
    "    y_true_k = y_test[:, 3*k : 3*k + 3]\n",
    "    y_pred_k = y_pred[:, 3*k : 3*k + 3]\n",
    "    est_k    = est_blocks[k]\n",
    "\n",
    "    err_raw_k = angle_error_deg(est_k, y_true_k)\n",
    "    err_ann_k = angle_error_deg(y_pred_k, y_true_k)\n",
    "\n",
    "    errs_raw.append(err_raw_k)\n",
    "    errs_ann.append(err_ann_k)\n",
    "\n",
    "print(\"\\n=== Angle Error Results (degrees) ===\")\n",
    "for k in range(K):\n",
    "    print(f\"TX{k} — RAW mean: {errs_raw[k].mean():.3f}, ANN mean: {errs_ann[k].mean():.3f}\")\n",
    "print(f\"Overall RAW: {np.hstack(errs_raw).mean():.3f}, ANN: {np.hstack(errs_ann).mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f0dde6-5f0b-42ff-aa58-fb8426327498",
   "metadata": {},
   "source": [
    "# Size Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89e2e6-e861-4c15-b475-1c5f691c77dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ------------------------------- Repro -------------------------------\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ------------------------------- Load & filter -------------------------------\n",
    "df = pd.read_csv(\"size_estimation_2Tx.csv\")\n",
    "df = df[df[\"method\"] == \"KMeans centers\"].copy()\n",
    "#df = df[df[\"method\"] == \"MCD-inlier density-weighted centers (on KMeans partitions)\"].copy()\n",
    "\n",
    "def to_vec3(x):\n",
    "    if isinstance(x, list): return [float(x[0]), float(x[1]), float(x[2])]\n",
    "    if isinstance(x, str):  return list(map(float, ast.literal_eval(x)))\n",
    "    raise ValueError(\"Unexpected est_center type\")\n",
    "\n",
    "df[\"est_center\"] = df[\"est_center\"].apply(to_vec3)\n",
    "df[\"est_size\"]   = df[\"est_size\"].astype(float)\n",
    "df[\"true_size\"]  = df[\"true_size\"].astype(float)\n",
    "df[\"tx_index\"]   = df[\"tx_index\"].astype(int)\n",
    "\n",
    "# ------------------------------- Infer K and build multi-cluster rows -------------------------------\n",
    "counts_per_file = df.groupby(\"file_index\")[\"tx_index\"].nunique()\n",
    "K = int(counts_per_file.mode().iloc[0])\n",
    "print(f\"Detected K = {K} clusters per scene (using modal count).\")\n",
    "\n",
    "def has_all_k(s):\n",
    "    return set(s[\"tx_index\"].unique()) == set(range(K))\n",
    "\n",
    "g = (df.groupby([\"file_index\"], as_index=False)\n",
    "       .filter(has_all_k)\n",
    "       .sort_values([\"file_index\", \"tx_index\"])\n",
    "       .reset_index(drop=True))\n",
    "\n",
    "rows = []\n",
    "for fid, s in g.groupby(\"file_index\"):\n",
    "    s = s.sort_values(\"tx_index\")\n",
    "    if list(s[\"tx_index\"]) != list(range(K)):\n",
    "        continue\n",
    "    feat = []\n",
    "    target = []\n",
    "    for k in range(K):\n",
    "        feat.append(float(s.loc[s[\"tx_index\"] == k, \"est_size\"].values[0]))\n",
    "        feat.extend(list(map(float, s.loc[s[\"tx_index\"] == k, \"est_center\"].values[0])))\n",
    "        target.append(float(s.loc[s[\"tx_index\"] == k, \"true_size\"].values[0]))\n",
    "    rows.append({\"file_index\": int(fid), \"X\": feat, \"y\": target})\n",
    "\n",
    "pairs = pd.DataFrame(rows)\n",
    "print(f\"Built {len(pairs)} training rows with K={K}.\")\n",
    "\n",
    "X_all = np.array(pairs[\"X\"].to_list(), dtype=float)      # (N, 4*K)\n",
    "y_all = np.array(pairs[\"y\"].to_list(), dtype=float)      # (N, K)\n",
    "\n",
    "# ------------------------------- Load consistent split -------------------------------\n",
    "SPLIT_CSV = \"train_val_test_split_2Tx.csv\"   # produced by first script\n",
    "split_df = pd.read_csv(SPLIT_CSV)\n",
    "train_files = set(split_df.loc[split_df[\"split\"] == \"train\", \"file_index\"].astype(int))\n",
    "val_files   = set(split_df.loc[split_df[\"split\"] == \"validation\", \"file_index\"].astype(int))\n",
    "test_files  = set(split_df.loc[split_df[\"split\"] == \"test\",  \"file_index\"].astype(int))\n",
    "\n",
    "pairs[\"file_index\"] = pairs[\"file_index\"].astype(int)\n",
    "pairs_train = pairs[pairs[\"file_index\"].isin(train_files)].copy()\n",
    "pairs_val   = pairs[pairs[\"file_index\"].isin(val_files)].copy()\n",
    "pairs_test  = pairs[pairs[\"file_index\"].isin(test_files)].copy()\n",
    "\n",
    "X_train = np.array(pairs_train[\"X\"].to_list(), dtype=float)\n",
    "y_train = np.array(pairs_train[\"y\"].to_list(), dtype=float)\n",
    "X_val   = np.array(pairs_val[\"X\"].to_list(), dtype=float)\n",
    "y_val   = np.array(pairs_val[\"y\"].to_list(), dtype=float)\n",
    "X_test  = np.array(pairs_test[\"X\"].to_list(),  dtype=float)\n",
    "y_test  = np.array(pairs_test[\"y\"].to_list(),  dtype=float)\n",
    "print(f\"Reused split — train N={len(X_train)}, val N={len(X_val)}, test N={len(X_test)}\")\n",
    "\n",
    "# ------------------------------- Scaling -------------------------------\n",
    "scaler_X_size = StandardScaler()\n",
    "X_train_s = scaler_X_size.fit_transform(X_train)\n",
    "X_val_s   = scaler_X_size.transform(X_val) \n",
    "X_test_s  = scaler_X_size.transform(X_test)\n",
    "\n",
    "y_mean_size = y_train.mean(axis=0)\n",
    "y_std_size  = y_train.std(axis=0); y_std_size[y_std_size==0] = 1.0\n",
    "y_train_s = (y_train - y_mean_size) / y_std_size\n",
    "y_val_s   = (y_val - y_mean_size) / y_std_size\n",
    "y_test_s  = (y_test  - y_mean_size) / y_std_size\n",
    "\n",
    "# ------------------------------- Dataset -------------------------------\n",
    "class MultiKDataset(Dataset):\n",
    "    def __init__(self, Xs, ys, K, permute_prob=0.5, train=True):\n",
    "        self.Xs = np.ascontiguousarray(Xs, dtype=np.float32)\n",
    "        self.ys = np.ascontiguousarray(ys, dtype=np.float32)\n",
    "        self.K = int(K)\n",
    "        self.permute_prob = float(permute_prob)\n",
    "        self.train = bool(train)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.Xs[idx].copy()\n",
    "        y = self.ys[idx].copy()\n",
    "        if self.train and np.random.rand() < self.permute_prob:\n",
    "            perm = np.random.permutation(self.K)\n",
    "            x = x.reshape(self.K, 4)[perm].reshape(-1)\n",
    "            y = y[perm]\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "train_ds = MultiKDataset(X_train_s, y_train_s, K=K, permute_prob=0.5, train=True)\n",
    "val_ds   = MultiKDataset(X_val_s,   y_val_s,   K=K, permute_prob=0.0, train=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
    "\n",
    "# ------------------------------- Residual MLP -------------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, p_drop=0.1, use_bn=False):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(dim, dim), nn.ReLU()]\n",
    "        if use_bn:\n",
    "            layers.append(nn.BatchNorm1d(dim))\n",
    "        layers += [nn.Linear(dim, dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.dropout = nn.Dropout(p_drop) if p_drop > 0 else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        return x + self.dropout(self.net(x))\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim=256, depth=6, p_drop=0.1, use_bn=False):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(nn.Linear(in_dim, hidden_dim), nn.ReLU())\n",
    "        self.blocks = nn.Sequential(*[ResidualBlock(hidden_dim, p_drop=p_drop, use_bn=use_bn) for _ in range(depth)])\n",
    "        self.head = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.head(self.blocks(self.stem(x)))\n",
    "\n",
    "in_dim = 4 * K\n",
    "out_dim = K\n",
    "size = ResidualMLP(in_dim=in_dim, out_dim=out_dim, hidden_dim=256, depth=6, p_drop=0.1)\n",
    "\n",
    "# ------------------------------- Optim & training -------------------------------\n",
    "opt = torch.optim.Adam(size.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "loss_fn = nn.SmoothL1Loss(beta=1.0)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=20)\n",
    "\n",
    "best_val, best_state, bad, patience = float(\"inf\"), None, 0, 60\n",
    "EPOCHS = 1000\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    size.train(); running = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        pred = size(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        running += loss.item() * xb.size(0)\n",
    "    train_loss = running / len(train_ds)\n",
    "\n",
    "    size.eval(); val_running = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            pred = size(xb)\n",
    "            val_running += loss_fn(pred, yb).item() * xb.size(0)\n",
    "    val_loss = val_running / len(val_ds)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    train_losses.append(train_loss); val_losses.append(val_loss)\n",
    "    if val_loss < best_val - 1e-6:\n",
    "        best_val, bad = val_loss, 0\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in size.state_dict().items()}\n",
    "    else:\n",
    "        bad += 1\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Train {train_loss:.6f} | Val {val_loss:.6f} | LR {opt.param_groups[0]['lr']:.2e}\")\n",
    "    if bad >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\"); break\n",
    "\n",
    "if best_state is not None:\n",
    "    size.load_state_dict(best_state)\n",
    "\n",
    "# ------------------------------- Predict & evaluate -------------------------------\n",
    "size.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_s = size(torch.from_numpy(X_test_s.astype(np.float32))).numpy()\n",
    "y_pred = y_pred_s * y_std_size + y_mean_size\n",
    "\n",
    "baseline_cols = [4*k for k in range(K)]\n",
    "baseline = X_test[:, baseline_cols]\n",
    "\n",
    "rmse_per_cluster_model, rmse_per_cluster_base = [], []\n",
    "for k in range(K):\n",
    "    rmse_per_cluster_model.append(float(np.sqrt(mean_squared_error(y_test[:,k], y_pred[:,k]))))\n",
    "    rmse_per_cluster_base.append(float(np.sqrt(mean_squared_error(y_test[:,k], baseline[:,k]))))\n",
    "\n",
    "rmse_overall_model = float(np.sqrt(mean_squared_error(y_test.reshape(-1), y_pred.reshape(-1))))\n",
    "rmse_overall_base  = float(np.sqrt(mean_squared_error(y_test.reshape(-1), baseline.reshape(-1))))\n",
    "\n",
    "for k in range(K):\n",
    "    delta = rmse_per_cluster_base[k] - rmse_per_cluster_model[k]\n",
    "    print(f\"RMSE TX{k} — ANN: {rmse_per_cluster_model[k]:.4f}   RAW: {rmse_per_cluster_base[k]:.4f}   Δ={delta:.4f}\")\n",
    "print(f\"RMSE Overall — ANN: {rmse_overall_model:.4f}   RAW: {rmse_overall_base:.4f}   Δ={rmse_overall_base - rmse_overall_model:.4f}\")\n",
    "print(f\"Best Val Loss (scaled SmoothL1): {best_val:.6f}\")\n",
    "\n",
    "# ------------------------------- MAPE -------------------------------\n",
    "eps = 1e-6\n",
    "for k in range(K):\n",
    "    mape_k = float(np.mean(np.abs((y_test[:,k] - y_pred[:,k]) / np.clip(np.abs(y_test[:,k]), eps, None))))\n",
    "    print(f\"MAPE TX{k} — ANN: {mape_k*100:.2f}%\")\n",
    "mape_overall = float(np.mean(np.abs((y_test - y_pred) / np.clip(np.abs(y_test), eps, None))))\n",
    "print(f\"MAPE Overall — ANN: {mape_overall*100:.2f}%\")\n",
    "\n",
    "# ------------------------------- Save artifacts -------------------------------\n",
    "globals()[\"scaler_X_size\"] = scaler_X_size\n",
    "globals()[\"y_mean_size\"] = y_mean_size\n",
    "globals()[\"y_std_size\"]  = y_std_size\n",
    "globals()[\"size\"] = size\n",
    "globals()[\"K\"] = K\n",
    "\n",
    "# ------------------------------- Plot losses -------------------------------\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(train_losses, label=\"Train\")\n",
    "    plt.plot(val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"SmoothL1 Loss\")\n",
    "    plt.legend(); plt.show()\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a784c-e8a4-4c44-a66f-8abb41189199",
   "metadata": {},
   "source": [
    "# ErfC Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4f21d-6e2a-4334-9cef-79028c186dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- STEP-BY-STEP COMPARISON PIPELINE ---------------\n",
    "# 1) Evaluate RAW with est_center_i & est_size_i\n",
    "# 2) Build features and feed models (size & angle)\n",
    "# 3) Obtain predicted results (counts & directions)\n",
    "# 4) Evaluate SIZE-ONLY, ANGLE-ONLY, ANGLE+SIZE\n",
    "#\n",
    "# This version generalizes to ANY number of clusters K.\n",
    "# - Features per scene: [size_k, cx_k, cy_k, cz_k] for k=0..K-1  -> 4*K dims\n",
    "# - Size model output: K counts (per cluster)\n",
    "# - Angle model output: 3*K vectors (per cluster), turned into unit directions\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.special import erfc\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "# ====================== Config ======================\n",
    "#METHOD = \"Density-weighted centers (on KMeans partitions)\"  # change/loop as needed\n",
    "METHOD = \"KMeans centers\"\n",
    "#METHOD = \"MCD-inlier density-weighted centers (on KMeans partitions)\"\n",
    "CFG_PATH = \"./dataset/config.csv\"\n",
    "DF_PATH  = \"size_estimation_2Tx.csv\"\n",
    "SPLIT_CSV = \"train_val_test_split_2Tx.csv\"\n",
    "\n",
    "N_EMITTED_PER_TX = 10000\n",
    "R_R_FIXED = 5.0\n",
    "D = 79.4\n",
    "T_HORIZON = 1.0\n",
    "F_EPS = 1e-12  # clamp for F in (0,1)\n",
    "\n",
    "# Optional: register additional models if their variable names differ\n",
    "MODEL_REGISTRY_SIZE = {\n",
    "    # \"size_v1\": dict(model=size,  scaler=scaler_X_size,  y_mean=y_mean_size,  y_std=y_std_size),\n",
    "    # \"size_v2\": dict(model=size2, scaler=scaler_X_size2, y_mean=y_mean_size2, y_std=y_std_size2),\n",
    "}\n",
    "MODEL_REGISTRY_ANGLE = {\n",
    "    # \"angle_v1\": dict(model=model,  scaler=scaler_X,  y_mean=y_mean,  y_std=y_std),\n",
    "    # \"angle_v2\": dict(model=model2, scaler=scaler_X2, y_mean=y_mean2, y_std=y_std2),\n",
    "}\n",
    "\n",
    "# ====================== Helpers ======================\n",
    "def to_vec3(x):\n",
    "    if isinstance(x, list): return [float(x[0]), float(x[1]), float(x[2])]\n",
    "    if isinstance(x, str):  return list(map(float, ast.literal_eval(x)))\n",
    "    raise ValueError(\"Unexpected 3D vector type\")\n",
    "\n",
    "def unit_vec(v, eps=1e-12):\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    n = float(np.linalg.norm(v))\n",
    "    if not np.isfinite(n) or n < eps:\n",
    "        return np.full(3, np.nan)\n",
    "    return v / n\n",
    "\n",
    "def vector_rmse(a, b):\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    return float(np.sqrt(np.mean((a - b) ** 2))) if len(a) and len(b) else np.nan\n",
    "\n",
    "def vector_mape(a, b, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Mean Absolute Percentage Error for 3D positions (in %).\n",
    "    Uses relative Euclidean error: ||est-true|| / ||true||.\n",
    "    Pairs with ||true|| <= eps are ignored.\n",
    "    \"\"\"\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    if a.size == 0 or b.size == 0:\n",
    "        return np.nan\n",
    "    A = a.reshape(-1, 3); B = b.reshape(-1, 3)\n",
    "    denom = np.linalg.norm(B, axis=1)\n",
    "    numer = np.linalg.norm(A - B, axis=1)\n",
    "    mask = denom > eps\n",
    "    if not np.any(mask):\n",
    "        return np.nan\n",
    "    return float(100.0 * np.mean(numer[mask] / denom[mask]))\n",
    "\n",
    "def invert_r0_from_fraction(F_hit, r_r, D, t, r0_min=None, r0_max=30):\n",
    "    \"\"\"Solve F = (r_r / r0) * erfc((r0-r_r)/sqrt(4Dt)) for r0; robust with clamp & bracket expansion.\"\"\"\n",
    "    if not np.isfinite(F_hit):\n",
    "        return np.nan\n",
    "    # Keep F within open interval (0,1) for a well-posed inverse\n",
    "    F_hit = float(np.clip(F_hit, F_EPS, 1.0 - F_EPS))\n",
    "\n",
    "    rr = float(r_r)\n",
    "    eps = 1e-9\n",
    "    a = rr + eps if r0_min is None else max(rr + eps, float(r0_min))  # physically r0 > r_r\n",
    "    b = float(r0_max)\n",
    "\n",
    "    def func(r0):\n",
    "        return (rr / r0) * erfc((r0 - rr) / np.sqrt(4.0 * D * t)) - F_hit\n",
    "\n",
    "    fa, fb = func(a), func(b)\n",
    "    tries = 0\n",
    "    # Expand the upper bracket if needed\n",
    "    while np.isfinite(fa) and np.isfinite(fb) and fa * fb > 0 and tries < 100:\n",
    "        b *= 2.0\n",
    "        fb = func(b)\n",
    "        tries += 1\n",
    "\n",
    "    if not (np.isfinite(fa) and np.isfinite(fb)) or fa * fb > 0:\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        return float(brentq(func, a, b, maxiter=10000))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def estimate_tx_from_cluster_r0(center_vec, n_received, N_emitted, D, t, r_r=5.0, rx_center=np.array([0.0,0.0,0.0])):\n",
    "    \"\"\"\n",
    "    Invert r0 from F = n_received / N_emitted, then place along direction(center_vec).\n",
    "    If n_received > N_emitted, estimate r0 = r_r directly (saturated case).\n",
    "    \"\"\"\n",
    "    # ---- Direction from receiver to measured cluster center ----\n",
    "    center_vec = np.asarray(center_vec, dtype=float)\n",
    "    rx_center  = np.asarray(rx_center,  dtype=float)\n",
    "    v = center_vec - rx_center\n",
    "    nv = np.linalg.norm(v)\n",
    "    if nv < 1e-12:\n",
    "        return np.array([np.nan, np.nan, np.nan]), np.nan, np.nan\n",
    "    u = v / nv\n",
    "\n",
    "    # ---- Validate counts ----\n",
    "    if not (np.isfinite(n_received) and np.isfinite(N_emitted) and N_emitted > 0):\n",
    "        return np.array([np.nan, np.nan, np.nan]), np.nan, np.nan\n",
    "\n",
    "    # ---- Saturation rule: more received than emitted -> r0 := r_r ----\n",
    "    if n_received > N_emitted:\n",
    "        r0 = float(r_r)\n",
    "        F_used = 1.0  # saturated fraction\n",
    "        tx_est = rx_center + r0 * u\n",
    "        return tx_est, r0, F_used\n",
    "\n",
    "    # ---- Regular path ----\n",
    "    # Clip to [1, N_emitted-1] to stay inside (0,1) after division\n",
    "    n_recv_clip = int(np.clip(int(round(float(n_received))), 1, int(N_emITTED_PER_TX := N_emitted) - 1))\n",
    "    F = float(n_recv_clip) / float(N_emitted)\n",
    "    F = float(np.clip(F, F_EPS, 1.0 - F_EPS))\n",
    "\n",
    "    r0 = invert_r0_from_fraction(F, r_r, D, t)\n",
    "    tx_est = rx_center + r0 * u if np.isfinite(r0) else np.array([0.0, 0.0, 0.0]) * np.nan\n",
    "    return tx_est, r0, F\n",
    "\n",
    "# ====================== Load data & build K-general pairs ======================\n",
    "df = pd.read_csv(DF_PATH)\n",
    "df = df[df[\"method\"] == METHOD].copy()\n",
    "df[\"est_center\"] = df[\"est_center\"].apply(to_vec3)\n",
    "df[\"est_size\"]   = df[\"est_size\"].astype(float)\n",
    "df[\"tx_index\"]   = df[\"tx_index\"].astype(int)\n",
    "\n",
    "cfg = pd.read_csv(CFG_PATH)\n",
    "if \"file_index\" not in cfg.columns:\n",
    "    cfg = cfg.reset_index().rename(columns={\"index\":\"file_index\"})\n",
    "\n",
    "# Build truth map and infer K (must be consistent across scenes)\n",
    "truth_map = {}\n",
    "K = None\n",
    "for _, r in cfg.iterrows():\n",
    "    centers = r[\"tx_centers\"]\n",
    "    centers = ast.literal_eval(centers) if isinstance(centers, str) else centers\n",
    "    centers = np.asarray([list(map(float, c)) for c in centers], dtype=float)\n",
    "    truth_map[int(r[\"file_index\"])] = centers\n",
    "    if K is None:\n",
    "        K = centers.shape[0]\n",
    "    else:\n",
    "        assert K == centers.shape[0], f\"Inconsistent K across files: got {centers.shape[0]} vs {K}\"\n",
    "\n",
    "print(f\"Detected K = {K} clusters per scene.\")\n",
    "\n",
    "# Keep only scenes present in truth and having all tx_index {0..K-1}\n",
    "df = df[df[\"file_index\"].isin(truth_map.keys())]\n",
    "\n",
    "def has_all_k(s):\n",
    "    return set(s[\"tx_index\"].unique()) == set(range(K))\n",
    "\n",
    "g = (df.groupby([\"file_index\"], as_index=False)\n",
    "       .filter(has_all_k)\n",
    "       .sort_values([\"file_index\",\"tx_index\"])\n",
    "       .reset_index(drop=True))\n",
    "\n",
    "# Build a compact \"pairs\" DataFrame with arrays per scene\n",
    "rows = []\n",
    "for fid, s in g.groupby(\"file_index\"):\n",
    "    s = s.sort_values(\"tx_index\")\n",
    "    if list(s[\"tx_index\"]) != list(range(K)):\n",
    "        continue\n",
    "    est_sizes = s[\"est_size\"].astype(float).to_numpy()                  # (K,)\n",
    "    est_centers = np.vstack(s[\"est_center\"].tolist()).astype(float)     # (K,3)\n",
    "    rows.append({\"file_index\": int(fid), \"est_sizes\": est_sizes, \"est_centers\": est_centers})\n",
    "\n",
    "pairs = pd.DataFrame(rows)\n",
    "if len(pairs) == 0:\n",
    "    raise ValueError(f\"No complete scenes with K={K} for method '{METHOD}'.\")\n",
    "print(f\"Prepared {len(pairs)} scenes (pre-split).\")\n",
    "\n",
    "# ====================== TEST SPLIT FILTER ======================\n",
    "split_df = pd.read_csv(SPLIT_CSV)\n",
    "test_files = set(split_df.loc[split_df[\"split\"].str.lower() == \"test\", \"file_index\"].astype(int))\n",
    "pairs = (pairs[pairs[\"file_index\"].isin(test_files)]\n",
    "         .sort_values(\"file_index\").reset_index(drop=True))\n",
    "if len(pairs) == 0:\n",
    "    raise ValueError(\"No test scenes after filtering by split CSV.\")\n",
    "print(f\"Using TEST set only — scenes: {len(pairs)}\")\n",
    "\n",
    "# ====================== Step 1 — Evaluate RAW ======================\n",
    "raw_est, raw_true = [], []\n",
    "raw_skipped = 0\n",
    "for _, r in pairs.iterrows():\n",
    "    T = np.asarray(truth_map[int(r[\"file_index\"])], dtype=float)  # (K,3)\n",
    "    E = []\n",
    "    for k in range(K):\n",
    "        c_vec = r[\"est_centers\"][k]\n",
    "        n_recv = r[\"est_sizes\"][k]\n",
    "        tx, _, _ = estimate_tx_from_cluster_r0(\n",
    "            center_vec=c_vec, n_received=float(n_recv),\n",
    "            N_emitted=N_EMITTED_PER_TX, D=D, t=T_HORIZON,\n",
    "            r_r=R_R_FIXED, rx_center=np.array([0.,0.,0.])\n",
    "        )\n",
    "        E.append(tx)\n",
    "    E = np.asarray(E, dtype=float)  # (K,3)\n",
    "    if not (np.all(np.isfinite(E)) and np.all(np.isfinite(T)) and E.shape==(K,3) and T.shape==(K,3)):\n",
    "        raw_skipped += 1\n",
    "        continue\n",
    "    cost = cdist(E, T, metric=\"euclidean\")\n",
    "    r_ind, c_ind = linear_sum_assignment(cost)\n",
    "    raw_est.extend(E[r_ind]); raw_true.extend(T[c_ind])\n",
    "\n",
    "rmse_raw = vector_rmse(raw_est, raw_true)\n",
    "mape_raw = vector_mape(raw_est, raw_true)\n",
    "print(f\"Step 1 — RAW | RMSE: {rmse_raw:.6f} | MAPE: {mape_raw:.3f}% | matches: {len(raw_est)} | skipped scenes: {raw_skipped}\")\n",
    "\n",
    "# ====================== Step 2 — Build features & discover models ======================\n",
    "def row_to_feat(r):\n",
    "    # Flatten blocks of 4 per cluster: [size_k, cx_k, cy_k, cz_k] for k=0..K-1\n",
    "    feat = []\n",
    "    for k in range(K):\n",
    "        feat.append(float(r[\"est_sizes\"][k]))\n",
    "        feat.extend(list(map(float, r[\"est_centers\"][k])))\n",
    "    return feat\n",
    "\n",
    "X4K = np.array([row_to_feat(r) for _, r in pairs.iterrows()], dtype=float)  # (N, 4*K)\n",
    "\n",
    "# Discover models in memory (or use registries filled above)\n",
    "def try_add_size_model(reg, name, model_name, scaler_name, mean_name, std_name, globs):\n",
    "    if all(k in globs for k in [model_name, scaler_name, mean_name, std_name]):\n",
    "        reg[name] = dict(model=globs[model_name], scaler=globs[scaler_name],\n",
    "                         y_mean=globs[mean_name], y_std=globs[std_name])\n",
    "\n",
    "def try_add_angle_model(reg, name, model_name, scaler_name, mean_name, std_name, globs):\n",
    "    if all(k in globs for k in [model_name, scaler_name, mean_name, std_name]):\n",
    "        reg[name] = dict(model=globs[model_name], scaler=globs[scaler_name],\n",
    "                         y_mean=globs[mean_name], y_std=globs[std_name])\n",
    "\n",
    "size_models = dict(MODEL_REGISTRY_SIZE)\n",
    "angle_models = dict(MODEL_REGISTRY_ANGLE)\n",
    "G = globals()\n",
    "try_add_size_model(size_models,  \"size_v1\", \"size\",  \"scaler_X_size\",  \"y_mean_size\",  \"y_std_size\",  G)\n",
    "try_add_size_model(size_models,  \"size_v2\", \"size2\", \"scaler_X_size2\", \"y_mean_size2\", \"y_std_size2\", G)\n",
    "try_add_angle_model(angle_models,\"angle_v1\",\"model\", \"scaler_X\",       \"y_mean\",       \"y_std\",       G)\n",
    "try_add_angle_model(angle_models,\"angle_v2\",\"model2\",\"scaler_X2\",      \"y_mean2\",      \"y_std2\",      G)\n",
    "\n",
    "print(f\"Step 2 — Models found | size: {list(size_models.keys()) or '(none)'} | angle: {list(angle_models.keys()) or '(none)'}\")\n",
    "\n",
    "# ====================== Step 3 — Predict (size counts & angle directions) ======================\n",
    "def predict_sizes(X, spec, K_expected):\n",
    "    mdl, scl, ym, ys = spec[\"model\"], spec[\"scaler\"], spec[\"y_mean\"], spec[\"y_std\"]\n",
    "    Xs = scl.transform(X)\n",
    "    with torch.no_grad():\n",
    "        y_s = mdl(torch.tensor(Xs, dtype=torch.float32)).cpu().numpy()  # (N, K_m)\n",
    "    y = y_s * ys + ym  # de-standardize\n",
    "    # Guard against mismatched K\n",
    "    if y.shape[1] != K_expected:\n",
    "        print(f\"[WARN] Skipping size model (out_dim={y.shape[1]} != K={K_expected}).\")\n",
    "        return None\n",
    "    # Return floats; downstream estimator clamps to physical range\n",
    "    return np.clip(y.astype(float), 1.0, float(N_EMITTED_PER_TX))\n",
    "\n",
    "def predict_angles(X, spec, K_expected):\n",
    "    mdl, scl, ym, ys = spec[\"model\"], spec[\"scaler\"], spec[\"y_mean\"], spec[\"y_std\"]\n",
    "    Xs = scl.transform(X)\n",
    "    with torch.no_grad():\n",
    "        y_s = mdl(torch.tensor(Xs, dtype=torch.float32)).cpu().numpy()  # (N, 3*K_m)\n",
    "    y = y_s * ys + ym\n",
    "    if y.shape[1] % 3 != 0:\n",
    "        print(f\"[WARN] Angle model output dim {y.shape[1]} not divisible by 3; skipping.\")\n",
    "        return None\n",
    "    K_m = y.shape[1] // 3\n",
    "    if K_m != K_expected:\n",
    "        print(f\"[WARN] Skipping angle model (K_out={K_m} != K={K_expected}).\")\n",
    "        return None\n",
    "    # Convert to unit directions (N, K, 3)\n",
    "    y = y.reshape(y.shape[0], K_m, 3)\n",
    "    dirs = np.zeros_like(y)\n",
    "    for i in range(y.shape[0]):\n",
    "        for k in range(K_m):\n",
    "            dirs[i, k] = unit_vec(y[i, k])\n",
    "    return dirs  # (N, K, 3)\n",
    "\n",
    "size_preds = {}\n",
    "for name, spec in size_models.items():\n",
    "    pred = predict_sizes(X4K, spec, K_expected=K)\n",
    "    if pred is not None:\n",
    "        size_preds[name] = pred\n",
    "\n",
    "angle_preds = {}\n",
    "for name, spec in angle_models.items():\n",
    "    pred = predict_angles(X4K, spec, K_expected=K)\n",
    "    if pred is not None:\n",
    "        angle_preds[name] = pred\n",
    "\n",
    "print(\"Step 3 — Predictions obtained.\")\n",
    "\n",
    "# ====================== Step 4 — Evaluate predicted variants ======================\n",
    "results = []\n",
    "\n",
    "# SIZE-ONLY (use raw dirs via est_centers, predicted counts)\n",
    "for name_sz, y_counts in size_preds.items():\n",
    "    est, tru = [], []\n",
    "    skipped = 0\n",
    "    for j, (_, r) in enumerate(pairs.iterrows()):\n",
    "        T = np.asarray(truth_map[int(r[\"file_index\"])], dtype=float)  # (K,3)\n",
    "        E = []\n",
    "        for k in range(K):\n",
    "            tx, _, _ = estimate_tx_from_cluster_r0(\n",
    "                center_vec=r[\"est_centers\"][k],\n",
    "                n_received=float(y_counts[j, k]),  # positional index!\n",
    "                N_emitted=N_EMITTED_PER_TX, D=D, t=T_HORIZON,\n",
    "                r_r=R_R_FIXED, rx_center=np.array([0.,0.,0.])\n",
    "            )\n",
    "            E.append(tx)\n",
    "        E = np.asarray(E, dtype=float)  # (K,3)\n",
    "        if not (np.all(np.isfinite(E)) and np.all(np.isfinite(T))):\n",
    "            skipped += 1\n",
    "            continue\n",
    "        cost = cdist(E, T, metric=\"euclidean\")\n",
    "        r_ind, c_ind = linear_sum_assignment(cost)\n",
    "        est.extend(E[r_ind]); tru.extend(T[c_ind])\n",
    "    rmse = vector_rmse(est, tru)\n",
    "    mape = vector_mape(est, tru)\n",
    "    print(f\"Step 4 — SIZE-ONLY[{name_sz}] | RMSE: {rmse:.6f} | MAPE: {mape:.3f}% | matches: {len(est)} | skipped scenes: {skipped}\")\n",
    "    results.append(dict(variant=\"SIZE-ONLY\", model=name_sz, rmse=rmse, mape=mape, matches=len(est)))\n",
    "\n",
    "# ANGLE-ONLY (use predicted dirs, observed counts to set radii via estimate_tx_from_cluster_r0)\n",
    "for name_ang, dirs in angle_preds.items():\n",
    "    est, tru = [], []\n",
    "    skipped = 0\n",
    "\n",
    "    for j, (_, r) in enumerate(pairs.iterrows()):\n",
    "        T = np.asarray(truth_map[int(r[\"file_index\"])], dtype=float)  # (K,3)\n",
    "        if T.shape[0] != K or not np.all(np.isfinite(T)):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        tx_estimates = []\n",
    "        ok = True\n",
    "\n",
    "        for k in range(K):\n",
    "            # direction for this cluster (ensure unit length)\n",
    "            dir_k = np.asarray(dirs[j][k], dtype=float)\n",
    "            n = np.linalg.norm(dir_k)\n",
    "            if not np.isfinite(n) or n < 1e-12:\n",
    "                ok = False\n",
    "                break\n",
    "            dir_k = dir_k / n\n",
    "\n",
    "            # observed count for this cluster\n",
    "            n_recv = float(r[\"est_sizes\"][k])\n",
    "\n",
    "            # estimate tx position along dir_k based on r0 inversion\n",
    "            tx_est, r0, F_used = estimate_tx_from_cluster_r0(\n",
    "                center_vec=dir_k,                 # just a direction; function normalizes anyway\n",
    "                n_received=n_recv,\n",
    "                N_emitted=N_EMITTED_PER_TX,\n",
    "                D=D,\n",
    "                t=T_HORIZON,\n",
    "                r_r=R_R_FIXED,\n",
    "                rx_center=np.array([0.0, 0.0, 0.0])  # receiver at origin\n",
    "            )\n",
    "\n",
    "            if not np.all(np.isfinite(tx_est)):\n",
    "                ok = False\n",
    "                break\n",
    "\n",
    "            tx_estimates.append(tx_est)\n",
    "\n",
    "        if not ok:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        E = np.vstack(tx_estimates)  # (K,3)\n",
    "        cost = cdist(E, T, metric=\"euclidean\")\n",
    "        r_ind, c_ind = linear_sum_assignment(cost)\n",
    "        est.extend(E[r_ind]); tru.extend(T[c_ind])\n",
    "\n",
    "    rmse = vector_rmse(est, tru)\n",
    "    mape = vector_mape(est, tru)\n",
    "    print(f\"Step 4 — ANGLE-ONLY[{name_ang}] | RMSE: {rmse:.6f} | MAPE: {mape:.3f}% | matches: {len(est)} | skipped scenes: {skipped}\")\n",
    "    results.append(dict(variant=\"ANGLE-ONLY\", model=name_ang, rmse=rmse, mape=mape, matches=len(est)))\n",
    "\n",
    "# ANGLE+SIZE (use predicted dirs and predicted counts via r0 inversion)\n",
    "for name_ang, dirs in angle_preds.items():\n",
    "    for name_sz, y_counts in size_preds.items():\n",
    "        est, tru = [], []\n",
    "        skipped = 0\n",
    "\n",
    "        for j, (_, r) in enumerate(pairs.iterrows()):\n",
    "            T = np.asarray(truth_map[int(r[\"file_index\"])], dtype=float)  # (K,3)\n",
    "            if T.shape[0] != K or not np.all(np.isfinite(T)):\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            tx_estimates = []\n",
    "            ok = True\n",
    "\n",
    "            for k in range(K):\n",
    "                # direction for this cluster -> ensure unit vector\n",
    "                dir_k = np.asarray(dirs[j][k], dtype=float)\n",
    "                nrm = np.linalg.norm(dir_k)\n",
    "                if not np.isfinite(nrm) or nrm < 1e-12:\n",
    "                    ok = False\n",
    "                    break\n",
    "                dir_k = dir_k / nrm\n",
    "\n",
    "                # predicted count for this cluster (float; function will clamp)\n",
    "                n_recv = float(y_counts[j, k])\n",
    "\n",
    "                # estimate tx position along dir_k based on r0 inversion\n",
    "                tx_est, r0, F_used = estimate_tx_from_cluster_r0(\n",
    "                    center_vec=dir_k,                 # direction is enough; function normalizes again\n",
    "                    n_received=n_recv,\n",
    "                    N_emitted=N_EMITTED_PER_TX,\n",
    "                    D=D,\n",
    "                    t=T_HORIZON,\n",
    "                    r_r=R_R_FIXED,\n",
    "                    rx_center=np.array([0.0, 0.0, 0.0])  # receiver at origin\n",
    "                )\n",
    "\n",
    "                if not np.all(np.isfinite(tx_est)):\n",
    "                    ok = False\n",
    "                    break\n",
    "\n",
    "                tx_estimates.append(tx_est)\n",
    "\n",
    "            if not ok:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            E = np.vstack(tx_estimates)  # (K,3)\n",
    "            cost = cdist(E, T, metric=\"euclidean\")\n",
    "            r_ind, c_ind = linear_sum_assignment(cost)\n",
    "            est.extend(E[r_ind]); tru.extend(T[c_ind])\n",
    "\n",
    "        rmse = vector_rmse(est, tru)\n",
    "        mape = vector_mape(est, tru)\n",
    "        print(f\"Step 4 — ANGLE+SIZE[{name_ang}×{name_sz}] | RMSE: {rmse:.6f} | MAPE: {mape:.3f}% | matches: {len(est)} | skipped scenes: {skipped}\")\n",
    "        results.append(dict(variant=\"ANGLE+SIZE\", model=f\"{name_ang} × {name_sz}\",\n",
    "                            rmse=rmse, mape=mape, matches=len(est)))\n",
    "\n",
    "# ====================== Summary & Save ======================\n",
    "summary = [dict(variant=\"RAW\", model=\"\", rmse=rmse_raw, mape=mape_raw, matches=len(raw_est))]\n",
    "summary.extend(results)\n",
    "res_df = pd.DataFrame(summary).sort_values([\"variant\",\"model\"]).reset_index(drop=True)\n",
    "print(\"\\n=== Summary (TEST only) ===\")\n",
    "print(res_df.to_string(index=False))\n",
    "\n",
    "out_csv = \"inversion_comparison_step_by_step_2Tx_TEST.csv\"\n",
    "res_df.to_csv(out_csv, index=False)\n",
    "print(f\"\\nSaved: {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d9926-8924-41d4-85e4-7e431cb54275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- STEP-BY-STEP COMPARISON PIPELINE ---------------\n",
    "# 1) Evaluate RAW with est_center_i & est_size_i\n",
    "# 2) Build features and feed models (size & angle)\n",
    "# 3) Obtain predicted results (counts & directions)\n",
    "# 4) Evaluate SIZE-ONLY, ANGLE-ONLY, ANGLE+SIZE\n",
    "#\n",
    "# This version generalizes to ANY number of clusters K.\n",
    "# - Features per scene: [size_k, cx_k, cy_k, cz_k] for k=0..K-1  -> 4*K dims\n",
    "# - Size model output: K counts (per cluster)\n",
    "# - Angle model output: 3*K vectors (per cluster), turned into unit directions\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.special import erfc\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "# ====================== Config ======================\n",
    "#METHOD = \"Density-weighted centers (on KMeans partitions)\"  # change/loop as needed\n",
    "METHOD = \"KMeans centers\"\n",
    "#METHOD = \"MCD-inlier density-weighted centers (on KMeans partitions)\"\n",
    "CFG_PATH = \"./uniform_test/config.csv\"\n",
    "DF_PATH  = \"size_estimation_2Tx.csv\"\n",
    "SPLIT_CSV = \"train_val_test_split_2Tx.csv\"\n",
    "\n",
    "N_EMITTED_PER_TX = 10000\n",
    "R_R_FIXED = 5.0\n",
    "D = 79.4\n",
    "T_HORIZON = 1.0\n",
    "F_EPS = 1e-12  # clamp for F in (0,1)\n",
    "\n",
    "# Optional: register additional models if their variable names differ\n",
    "MODEL_REGISTRY_SIZE = {\n",
    "    # \"size_v1\": dict(model=size,  scaler=scaler_X_size,  y_mean=y_mean_size,  y_std=y_std_size),\n",
    "    # \"size_v2\": dict(model=size2, scaler=scaler_X_size2, y_mean=y_mean_size2, y_std=y_std_size2),\n",
    "}\n",
    "MODEL_REGISTRY_ANGLE = {\n",
    "    # \"angle_v1\": dict(model=model,  scaler=scaler_X,  y_mean=y_mean,  y_std=y_std),\n",
    "    # \"angle_v2\": dict(model=model2, scaler=scaler_X2, y_mean=y_mean2, y_std=y_std2),\n",
    "}\n",
    "\n",
    "# ====================== Helpers ======================\n",
    "def to_vec3(x):\n",
    "    if isinstance(x, list): return [float(x[0]), float(x[1]), float(x[2])]\n",
    "    if isinstance(x, str):  return list(map(float, ast.literal_eval(x)))\n",
    "    raise ValueError(\"Unexpected 3D vector type\")\n",
    "\n",
    "def unit_vec(v, eps=1e-12):\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    n = float(np.linalg.norm(v))\n",
    "    if not np.isfinite(n) or n < eps:\n",
    "        return np.full(3, np.nan)\n",
    "    return v / n\n",
    "\n",
    "def vector_rmse(a, b):\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    return float(np.sqrt(np.mean((a - b) ** 2))) if len(a) and len(b) else np.nan\n",
    "\n",
    "def vector_mape(a, b, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Mean Absolute Percentage Error for 3D positions (in %).\n",
    "    Uses relative Euclidean error: ||est-true|| / ||true||.\n",
    "    Pairs with ||true|| <= eps are ignored.\n",
    "    \"\"\"\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    if a.size == 0 or b.size == 0:\n",
    "        return np.nan\n",
    "    A = a.reshape(-1, 3); B = b.reshape(-1, 3)\n",
    "    denom = np.linalg.norm(B, axis=1)\n",
    "    numer = np.linalg.norm(A - B, axis=1)\n",
    "    mask = denom > eps\n",
    "    if not np.any(mask):\n",
    "        return np.nan\n",
    "    return float(100.0 * np.mean(numer[mask] / denom[mask]))\n",
    "\n",
    "def invert_r0_from_fraction(F_hit, r_r, D, t, r0_min=None, r0_max=30):\n",
    "    \"\"\"Solve F = (r_r / r0) * erfc((r0-r_r)/sqrt(4Dt)) for r0; robust with clamp & bracket expansion.\"\"\"\n",
    "    if not np.isfinite(F_hit):\n",
    "        return np.nan\n",
    "    # Keep F within open interval (0,1) for a well-posed inverse\n",
    "    F_hit = float(np.clip(F_hit, F_EPS, 1.0 - F_EPS))\n",
    "\n",
    "    rr = float(r_r)\n",
    "    eps = 1e-9\n",
    "    a = rr + eps if r0_min is None else max(rr + eps, float(r0_min))  # physically r0 > r_r\n",
    "    b = float(r0_max)\n",
    "\n",
    "    def func(r0):\n",
    "        return (rr / r0) * erfc((r0 - rr) / np.sqrt(4.0 * D * t)) - F_hit\n",
    "\n",
    "    fa, fb = func(a), func(b)\n",
    "    tries = 0\n",
    "    # Expand the upper bracket if needed\n",
    "    while np.isfinite(fa) and np.isfinite(fb) and fa * fb > 0 and tries < 100:\n",
    "        b *= 2.0\n",
    "        fb = func(b)\n",
    "        tries += 1\n",
    "\n",
    "    if not (np.isfinite(fa) and np.isfinite(fb)) or fa * fb > 0:\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        return float(brentq(func, a, b, maxiter=10000))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def estimate_tx_from_cluster_r0(center_vec, n_received, N_emitted, D, t, r_r=5.0, rx_center=np.array([0.0,0.0,0.0])):\n",
    "    \"\"\"\n",
    "    Invert r0 from F = n_received / N_emitted, then place along direction(center_vec).\n",
    "    If n_received > N_emitted, estimate r0 = r_r directly (saturated case).\n",
    "    \"\"\"\n",
    "    # ---- Direction from receiver to measured cluster center ----\n",
    "    center_vec = np.asarray(center_vec, dtype=float)\n",
    "    rx_center  = np.asarray(rx_center,  dtype=float)\n",
    "    v = center_vec - rx_center\n",
    "    nv = np.linalg.norm(v)\n",
    "    if nv < 1e-12:\n",
    "        return np.array([np.nan, np.nan, np.nan]), np.nan, np.nan\n",
    "    u = v / nv\n",
    "\n",
    "    # ---- Validate counts ----\n",
    "    if not (np.isfinite(n_received) and np.isfinite(N_emitted) and N_emitted > 0):\n",
    "        return np.array([np.nan, np.nan, np.nan]), np.nan, np.nan\n",
    "\n",
    "    # ---- Saturation rule: more received than emitted -> r0 := r_r ----\n",
    "    if n_received > N_emitted:\n",
    "        r0 = float(r_r)\n",
    "        F_used = 1.0  # saturated fraction\n",
    "        tx_est = rx_center + r0 * u\n",
    "        return tx_est, r0, F_used\n",
    "\n",
    "    # ---- Regular path ----\n",
    "    # Clip to [1, N_emitted-1] to stay inside (0,1) after division\n",
    "    n_recv_clip = int(np.clip(int(round(float(n_received))), 1, int(N_emITTED_PER_TX := N_emitted) - 1))\n",
    "    F = float(n_recv_clip) / float(N_emitted)\n",
    "    F = float(np.clip(F, F_EPS, 1.0 - F_EPS))\n",
    "\n",
    "    r0 = invert_r0_from_fraction(F, r_r, D, t)\n",
    "    tx_est = rx_center + r0 * u if np.isfinite(r0) else np.array([0.0, 0.0, 0.0]) * np.nan\n",
    "    return tx_est, r0, F\n",
    "\n",
    "# ====================== Load data & build K-general pairs ======================\n",
    "df = pd.read_csv(DF_PATH)\n",
    "df = df[df[\"method\"] == METHOD].copy()\n",
    "df[\"est_center\"] = df[\"est_center\"].apply(to_vec3)\n",
    "df[\"est_size\"]   = df[\"est_size\"].astype(float)\n",
    "df[\"tx_index\"]   = df[\"tx_index\"].astype(int)\n",
    "\n",
    "cfg = pd.read_csv(CFG_PATH)\n",
    "if \"file_index\" not in cfg.columns:\n",
    "    cfg = cfg.reset_index().rename(columns={\"index\":\"file_index\"})\n",
    "\n",
    "# Build truth map and infer K (must be consistent across scenes)\n",
    "truth_map = {}\n",
    "K = None\n",
    "for _, r in cfg.iterrows():\n",
    "    centers = r[\"tx_centers\"]\n",
    "    centers = ast.literal_eval(centers) if isinstance(centers, str) else centers\n",
    "    centers = np.asarray([list(map(float, c)) for c in centers], dtype=float)\n",
    "    truth_map[int(r[\"file_index\"])] = centers\n",
    "    if K is None:\n",
    "        K = centers.shape[0]\n",
    "    else:\n",
    "        assert K == centers.shape[0], f\"Inconsistent K across files: got {centers.shape[0]} vs {K}\"\n",
    "\n",
    "print(f\"Detected K = {K} clusters per scene.\")\n",
    "\n",
    "# Keep only scenes present in truth and having all tx_index {0..K-1}\n",
    "df = df[df[\"file_index\"].isin(truth_map.keys())]\n",
    "\n",
    "def has_all_k(s):\n",
    "    return set(s[\"tx_index\"].unique()) == set(range(K))\n",
    "\n",
    "g = (df.groupby([\"file_index\"], as_index=False)\n",
    "       .filter(has_all_k)\n",
    "       .sort_values([\"file_index\",\"tx_index\"])\n",
    "       .reset_index(drop=True))\n",
    "\n",
    "# Build a compact \"pairs\" DataFrame with arrays per scene\n",
    "rows = []\n",
    "for fid, s in g.groupby(\"file_index\"):\n",
    "    s = s.sort_values(\"tx_index\")\n",
    "    if list(s[\"tx_index\"]) != list(range(K)):\n",
    "        continue\n",
    "    est_sizes = s[\"est_size\"].astype(float).to_numpy()                  # (K,)\n",
    "    est_centers = np.vstack(s[\"est_center\"].tolist()).astype(float)     # (K,3)\n",
    "    rows.append({\"file_index\": int(fid), \"est_sizes\": est_sizes, \"est_centers\": est_centers})\n",
    "\n",
    "pairs = pd.DataFrame(rows)\n",
    "if len(pairs) == 0:\n",
    "    raise ValueError(f\"No complete scenes with K={K} for method '{METHOD}'.\")\n",
    "print(f\"Prepared {len(pairs)} scenes (pre-split).\")\n",
    "\n",
    "# ====================== TEST SPLIT FILTER ======================\n",
    "split_df = pd.read_csv(SPLIT_CSV)\n",
    "test_files = set(split_df.loc[split_df[\"split\"].str.lower() == \"test\", \"file_index\"].astype(int))\n",
    "pairs = (pairs[pairs[\"file_index\"].isin(test_files)]\n",
    "         .sort_values(\"file_index\").reset_index(drop=True))\n",
    "if len(pairs) == 0:\n",
    "    raise ValueError(\"No test scenes after filtering by split CSV.\")\n",
    "print(f\"Using TEST set only — scenes: {len(pairs)}\")\n",
    "\n",
    "# ====================== Step 1 — Evaluate RAW ======================\n",
    "raw_est, raw_true = [], []\n",
    "raw_skipped = 0\n",
    "for _, r in pairs.iterrows():\n",
    "    T = np.asarray(truth_map[int(r[\"file_index\"])], dtype=float)  # (K,3)\n",
    "    E = []\n",
    "    for k in range(K):\n",
    "        c_vec = r[\"est_centers\"][k]\n",
    "        n_recv = r[\"est_sizes\"][k]\n",
    "        tx, _, _ = estimate_tx_from_cluster_r0(\n",
    "            center_vec=c_vec, n_received=float(n_recv),\n",
    "            N_emitted=N_EMITTED_PER_TX, D=D, t=T_HORIZON,\n",
    "            r_r=R_R_FIXED, rx_center=np.array([0.,0.,0.])\n",
    "        )\n",
    "        E.append(tx)\n",
    "    E = np.asarray(E, dtype=float)  # (K,3)\n",
    "    if not (np.all(np.isfinite(E)) and np.all(np.isfinite(T)) and E.shape==(K,3) and T.shape==(K,3)):\n",
    "        raw_skipped += 1\n",
    "        continue\n",
    "    cost = cdist(E, T, metric=\"euclidean\")\n",
    "    r_ind, c_ind = linear_sum_assignment(cost)\n",
    "    raw_est.extend(E[r_ind]); raw_true.extend(T[c_ind])\n",
    "\n",
    "rmse_raw = vector_rmse(raw_est, raw_true)\n",
    "mape_raw = vector_mape(raw_est, raw_true)\n",
    "print(f\"Step 1 — RAW | RMSE: {rmse_raw:.6f} | MAPE: {mape_raw:.3f}% | matches: {len(raw_est)} | skipped scenes: {raw_skipped}\")\n",
    "\n",
    "# ====================== Step 2 — Build features & discover models ======================\n",
    "def row_to_feat(r):\n",
    "    # Flatten blocks of 4 per cluster: [size_k, cx_k, cy_k, cz_k] for k=0..K-1\n",
    "    feat = []\n",
    "    for k in range(K):\n",
    "        feat.append(float(r[\"est_sizes\"][k]))\n",
    "        feat.extend(list(map(float, r[\"est_centers\"][k])))\n",
    "    return feat\n",
    "\n",
    "X4K = np.array([row_to_feat(r) for _, r in pairs.iterrows()], dtype=float)  # (N, 4*K)\n",
    "\n",
    "# Discover models in memory (or use registries filled above)\n",
    "def try_add_size_model(reg, name, model_name, scaler_name, mean_name, std_name, globs):\n",
    "    if all(k in globs for k in [model_name, scaler_name, mean_name, std_name]):\n",
    "        reg[name] = dict(model=globs[model_name], scaler=globs[scaler_name],\n",
    "                         y_mean=globs[mean_name], y_std=globs[std_name])\n",
    "\n",
    "def try_add_angle_model(reg, name, model_name, scaler_name, mean_name, std_name, globs):\n",
    "    if all(k in globs for k in [model_name, scaler_name, mean_name, std_name]):\n",
    "        reg[name] = dict(model=globs[model_name], scaler=globs[scaler_name],\n",
    "                         y_mean=globs[mean_name], y_std=globs[std_name])\n",
    "\n",
    "size_models = dict(MODEL_REGISTRY_SIZE)\n",
    "angle_models = dict(MODEL_REGISTRY_ANGLE)\n",
    "G = globals()\n",
    "try_add_size_model(size_models,  \"size_v1\", \"size\",  \"scaler_X_size\",  \"y_mean_size\",  \"y_std_size\",  G)\n",
    "try_add_size_model(size_models,  \"size_v2\", \"size2\", \"scaler_X_size2\", \"y_mean_size2\", \"y_std_size2\", G)\n",
    "try_add_angle_model(angle_models,\"angle_v1\",\"model\", \"scaler_X\",       \"y_mean\",       \"y_std\",       G)\n",
    "try_add_angle_model(angle_models,\"angle_v2\",\"model2\",\"scaler_X2\",      \"y_mean2\",      \"y_std2\",      G)\n",
    "\n",
    "print(f\"Step 2 — Models found | size: {list(size_models.keys()) or '(none)'} | angle: {list(angle_models.keys()) or '(none)'}\")\n",
    "\n",
    "# ====================== Step 3 — Predict (size counts & angle directions) ======================\n",
    "def predict_sizes(X, spec, K_expected):\n",
    "    mdl, scl, ym, ys = spec[\"model\"], spec[\"scaler\"], spec[\"y_mean\"], spec[\"y_std\"]\n",
    "    Xs = scl.transform(X)\n",
    "    with torch.no_grad():\n",
    "        y_s = mdl(torch.tensor(Xs, dtype=torch.float32)).cpu().numpy()  # (N, K_m)\n",
    "    y = y_s * ys + ym  # de-standardize\n",
    "    # Guard against mismatched K\n",
    "    if y.shape[1] != K_expected:\n",
    "        print(f\"[WARN] Skipping size model (out_dim={y.shape[1]} != K={K_expected}).\")\n",
    "        return None\n",
    "    # Return floats; downstream estimator clamps to physical range\n",
    "    return np.clip(y.astype(float), 1.0, float(N_EMITTED_PER_TX))\n",
    "\n",
    "def predict_angles(X, spec, K_expected):\n",
    "    mdl, scl, ym, ys = spec[\"model\"], spec[\"scaler\"], spec[\"y_mean\"], spec[\"y_std\"]\n",
    "    Xs = scl.transform(X)\n",
    "    with torch.no_grad():\n",
    "        y_s = mdl(torch.tensor(Xs, dtype=torch.float32)).cpu().numpy()  # (N, 3*K_m)\n",
    "    y = y_s * ys + ym\n",
    "    if y.shape[1] % 3 != 0:\n",
    "        print(f\"[WARN] Angle model output dim {y.shape[1]} not divisible by 3; skipping.\")\n",
    "        return None\n",
    "    K_m = y.shape[1] // 3\n",
    "    if K_m != K_expected:\n",
    "        print(f\"[WARN] Skipping angle model (K_out={K_m} != K={K_expected}).\")\n",
    "        return None\n",
    "    # Convert to unit directions (N, K, 3)\n",
    "    y = y.reshape(y.shape[0], K_m, 3)\n",
    "    dirs = np.zeros_like(y)\n",
    "    for i in range(y.shape[0]):\n",
    "        for k in range(K_m):\n",
    "            dirs[i, k] = unit_vec(y[i, k])\n",
    "    return dirs  # (N, K, 3)\n",
    "\n",
    "size_preds = {}\n",
    "for name, spec in size_models.items():\n",
    "    pred = predict_sizes(X4K, spec, K_expected=K)\n",
    "    if pred is not None:\n",
    "        size_preds[name] = pred\n",
    "\n",
    "angle_preds = {}\n",
    "for name, spec in angle_models.items():\n",
    "    pred = predict_angles(X4K, spec, K_expected=K)\n",
    "    if pred is not None:\n",
    "        angle_preds[name] = pred\n",
    "\n",
    "print(\"Step 3 — Predictions obtained.\")\n",
    "\n",
    "# ====================== Step 4 — Evaluate predicted variants ======================\n",
    "results = []\n",
    "\n",
    "# SIZE-ONLY (use raw dirs via est_centers, predicted counts)\n",
    "for name_sz, y_counts in size_preds.items():\n",
    "    est, tru = [], []\n",
    "    skipped = 0\n",
    "    for j, (_, r) in enumerate(pairs.iterrows()):\n",
    "        T = np.asarray(truth_map[int(r[\"file_index\"])], dtype=float)  # (K,3)\n",
    "        E = []\n",
    "        for k in range(K):\n",
    "            tx, _, _ = estimate_tx_from_cluster_r0(\n",
    "                center_vec=r[\"est_centers\"][k],\n",
    "                n_received=float(y_counts[j, k]),  # positional index!\n",
    "                N_emitted=N_EMITTED_PER_TX, D=D, t=T_HORIZON,\n",
    "                r_r=R_R_FIXED, rx_center=np.array([0.,0.,0.])\n",
    "            )\n",
    "            E.append(tx)\n",
    "        E = np.asarray(E, dtype=float)  # (K,3)\n",
    "        if not (np.all(np.isfinite(E)) and np.all(np.isfinite(T))):\n",
    "            skipped += 1\n",
    "            continue\n",
    "        cost = cdist(E, T, metric=\"euclidean\")\n",
    "        r_ind, c_ind = linear_sum_assignment(cost)\n",
    "        est.extend(E[r_ind]); tru.extend(T[c_ind])\n",
    "    rmse = vector_rmse(est, tru)\n",
    "    mape = vector_mape(est, tru)\n",
    "    print(f\"Step 4 — SIZE-ONLY[{name_sz}] | RMSE: {rmse:.6f} | MAPE: {mape:.3f}% | matches: {len(est)} | skipped scenes: {skipped}\")\n",
    "    results.append(dict(variant=\"SIZE-ONLY\", model=name_sz, rmse=rmse, mape=mape, matches=len(est)))\n",
    "\n",
    "# ANGLE-ONLY (use predicted dirs, observed counts to set radii via estimate_tx_from_cluster_r0)\n",
    "for name_ang, dirs in angle_preds.items():\n",
    "    est, tru = [], []\n",
    "    skipped = 0\n",
    "\n",
    "    for j, (_, r) in enumerate(pairs.iterrows()):\n",
    "        T = np.asarray(truth_map[int(r[\"file_index\"])], dtype=float)  # (K,3)\n",
    "        if T.shape[0] != K or not np.all(np.isfinite(T)):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        tx_estimates = []\n",
    "        ok = True\n",
    "\n",
    "        for k in range(K):\n",
    "            # direction for this cluster (ensure unit length)\n",
    "            dir_k = np.asarray(dirs[j][k], dtype=float)\n",
    "            n = np.linalg.norm(dir_k)\n",
    "            if not np.isfinite(n) or n < 1e-12:\n",
    "                ok = False\n",
    "                break\n",
    "            dir_k = dir_k / n\n",
    "\n",
    "            # observed count for this cluster\n",
    "            n_recv = float(r[\"est_sizes\"][k])\n",
    "\n",
    "            # estimate tx position along dir_k based on r0 inversion\n",
    "            tx_est, r0, F_used = estimate_tx_from_cluster_r0(\n",
    "                center_vec=dir_k,                 # just a direction; function normalizes anyway\n",
    "                n_received=n_recv,\n",
    "                N_emitted=N_EMITTED_PER_TX,\n",
    "                D=D,\n",
    "                t=T_HORIZON,\n",
    "                r_r=R_R_FIXED,\n",
    "                rx_center=np.array([0.0, 0.0, 0.0])  # receiver at origin\n",
    "            )\n",
    "\n",
    "            if not np.all(np.isfinite(tx_est)):\n",
    "                ok = False\n",
    "                break\n",
    "\n",
    "            tx_estimates.append(tx_est)\n",
    "\n",
    "        if not ok:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        E = np.vstack(tx_estimates)  # (K,3)\n",
    "        cost = cdist(E, T, metric=\"euclidean\")\n",
    "        r_ind, c_ind = linear_sum_assignment(cost)\n",
    "        est.extend(E[r_ind]); tru.extend(T[c_ind])\n",
    "\n",
    "    rmse = vector_rmse(est, tru)\n",
    "    mape = vector_mape(est, tru)\n",
    "    print(f\"Step 4 — ANGLE-ONLY[{name_ang}] | RMSE: {rmse:.6f} | MAPE: {mape:.3f}% | matches: {len(est)} | skipped scenes: {skipped}\")\n",
    "    results.append(dict(variant=\"ANGLE-ONLY\", model=name_ang, rmse=rmse, mape=mape, matches=len(est)))\n",
    "\n",
    "# ANGLE+SIZE (use predicted dirs and predicted counts via r0 inversion)\n",
    "for name_ang, dirs in angle_preds.items():\n",
    "    for name_sz, y_counts in size_preds.items():\n",
    "        est, tru = [], []\n",
    "        skipped = 0\n",
    "\n",
    "        for j, (_, r) in enumerate(pairs.iterrows()):\n",
    "            T = np.asarray(truth_map[int(r[\"file_index\"])], dtype=float)  # (K,3)\n",
    "            if T.shape[0] != K or not np.all(np.isfinite(T)):\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            tx_estimates = []\n",
    "            ok = True\n",
    "\n",
    "            for k in range(K):\n",
    "                # direction for this cluster -> ensure unit vector\n",
    "                dir_k = np.asarray(dirs[j][k], dtype=float)\n",
    "                nrm = np.linalg.norm(dir_k)\n",
    "                if not np.isfinite(nrm) or nrm < 1e-12:\n",
    "                    ok = False\n",
    "                    break\n",
    "                dir_k = dir_k / nrm\n",
    "\n",
    "                # predicted count for this cluster (float; function will clamp)\n",
    "                n_recv = float(y_counts[j, k])\n",
    "\n",
    "                # estimate tx position along dir_k based on r0 inversion\n",
    "                tx_est, r0, F_used = estimate_tx_from_cluster_r0(\n",
    "                    center_vec=dir_k,                 # direction is enough; function normalizes again\n",
    "                    n_received=n_recv,\n",
    "                    N_emitted=N_EMITTED_PER_TX,\n",
    "                    D=D,\n",
    "                    t=T_HORIZON,\n",
    "                    r_r=R_R_FIXED,\n",
    "                    rx_center=np.array([0.0, 0.0, 0.0])  # receiver at origin\n",
    "                )\n",
    "\n",
    "                if not np.all(np.isfinite(tx_est)):\n",
    "                    ok = False\n",
    "                    break\n",
    "\n",
    "                tx_estimates.append(tx_est)\n",
    "\n",
    "            if not ok:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            E = np.vstack(tx_estimates)  # (K,3)\n",
    "            cost = cdist(E, T, metric=\"euclidean\")\n",
    "            r_ind, c_ind = linear_sum_assignment(cost)\n",
    "            est.extend(E[r_ind]); tru.extend(T[c_ind])\n",
    "\n",
    "        rmse = vector_rmse(est, tru)\n",
    "        mape = vector_mape(est, tru)\n",
    "        print(f\"Step 4 — ANGLE+SIZE[{name_ang}×{name_sz}] | RMSE: {rmse:.6f} | MAPE: {mape:.3f}% | matches: {len(est)} | skipped scenes: {skipped}\")\n",
    "        results.append(dict(variant=\"ANGLE+SIZE\", model=f\"{name_ang} × {name_sz}\",\n",
    "                            rmse=rmse, mape=mape, matches=len(est)))\n",
    "\n",
    "# ====================== Summary & Save ======================\n",
    "summary = [dict(variant=\"RAW\", model=\"\", rmse=rmse_raw, mape=mape_raw, matches=len(raw_est))]\n",
    "summary.extend(results)\n",
    "res_df = pd.DataFrame(summary).sort_values([\"variant\",\"model\"]).reset_index(drop=True)\n",
    "print(\"\\n=== Summary (TEST only) ===\")\n",
    "print(res_df.to_string(index=False))\n",
    "\n",
    "out_csv = \"inversion_comparison_step_by_step_2Tx_TEST.csv\" \n",
    "res_df.to_csv(out_csv, index=False)\n",
    "print(f\"\\nSaved: {out_csv}\")\n",
    "\n",
    "# ====================== EXTRA: RAW results for OTHER methods (TEST only) ======================\n",
    "# This computes RAW (Step 1) for every other method present in DF_PATH and saves a separate CSV.\n",
    "df_all = pd.read_csv(DF_PATH).copy()\n",
    "df_all = df_all[df_all[\"file_index\"].isin(truth_map.keys())]\n",
    "\n",
    "# parse/clean needed columns\n",
    "df_all[\"est_center\"] = df_all[\"est_center\"].apply(to_vec3)\n",
    "df_all[\"est_size\"]   = pd.to_numeric(df_all[\"est_size\"], errors=\"coerce\")\n",
    "df_all[\"tx_index\"]   = pd.to_numeric(df_all[\"tx_index\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "selected_methods = [\n",
    "    \"Density-weighted centers (on KMeans partitions)\",\n",
    "    \"MCD-inlier density-weighted centers (on KMeans partitions)\",\n",
    "    \"Robust (MinCovDet) centers (on KMeans partitions)\",  \n",
    "]\n",
    "raw_rows = []\n",
    "for meth in selected_methods:\n",
    "    dm = df_all[df_all[\"method\"] == meth].dropna(subset=[\"tx_index\"]).copy()\n",
    "    dm[\"tx_index\"] = dm[\"tx_index\"].astype(int)\n",
    "\n",
    "    # ensure each file has all K clusters\n",
    "    gm = (dm.groupby([\"file_index\"], as_index=False)\n",
    "            .filter(has_all_k)\n",
    "            .sort_values([\"file_index\",\"tx_index\"])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "    # build pairs for this method\n",
    "    rows_m = []\n",
    "    for fid, s in gm.groupby(\"file_index\"):\n",
    "        s = s.sort_values(\"tx_index\")\n",
    "        if list(s[\"tx_index\"]) != list(range(K)):\n",
    "            continue\n",
    "        est_sizes = s[\"est_size\"].astype(float).to_numpy()\n",
    "        est_centers = np.vstack(s[\"est_center\"].tolist()).astype(float)\n",
    "        rows_m.append({\"file_index\": int(fid), \"est_sizes\": est_sizes, \"est_centers\": est_centers})\n",
    "    pairs_m = pd.DataFrame(rows_m)\n",
    "    if len(pairs_m) == 0:\n",
    "        print(f\"[RAW-OTHER] {meth}: no complete scenes.\")\n",
    "        continue\n",
    "\n",
    "    # TEST split filter\n",
    "    pairs_m = pairs_m[pairs_m[\"file_index\"].isin(test_files)].sort_values(\"file_index\").reset_index(drop=True)\n",
    "    if len(pairs_m) == 0:\n",
    "        print(f\"[RAW-OTHER] {meth}: no TEST scenes after split.\")\n",
    "        continue\n",
    "\n",
    "    # Step 1 RAW for this method\n",
    "    est_m, tru_m = [], []\n",
    "    skipped_m = 0\n",
    "    for _, rr in pairs_m.iterrows():\n",
    "        T = np.asarray(truth_map[int(rr[\"file_index\"])], dtype=float)  # (K,3)\n",
    "        E = []\n",
    "        for k in range(K):\n",
    "            c_vec = rr[\"est_centers\"][k]\n",
    "            n_recv = rr[\"est_sizes\"][k]\n",
    "            tx, _, _ = estimate_tx_from_cluster_r0(\n",
    "                center_vec=c_vec, n_received=float(n_recv),\n",
    "                N_emitted=N_EMITTED_PER_TX, D=D, t=T_HORIZON,\n",
    "                r_r=R_R_FIXED, rx_center=np.array([0.,0.,0.])\n",
    "            )\n",
    "            E.append(tx)\n",
    "        E = np.asarray(E, dtype=float)\n",
    "        if not (np.all(np.isfinite(E)) and np.all(np.isfinite(T)) and E.shape==(K,3) and T.shape==(K,3)):\n",
    "            skipped_m += 1\n",
    "            continue\n",
    "        cost = cdist(E, T, metric=\"euclidean\")\n",
    "        r_ind, c_ind = linear_sum_assignment(cost)\n",
    "        est_m.extend(E[r_ind]); tru_m.extend(T[c_ind])\n",
    "\n",
    "    rmse_m = vector_rmse(est_m, tru_m)\n",
    "    mape_m = vector_mape(est_m, tru_m)\n",
    "    print(f\"[RAW-OTHER] {meth} | RMSE: {rmse_m:.6f} | MAPE: {mape_m:.3f}% | matches: {len(est_m)} | skipped scenes: {skipped_m}\")\n",
    "    raw_rows.append(dict(method=meth, rmse=rmse_m, mape=mape_m, matches=len(est_m), skipped=skipped_m))\n",
    "\n",
    "raw_df = pd.DataFrame(raw_rows).sort_values([\"rmse\",\"method\"]).reset_index(drop=True)\n",
    "raw_out_csv = \"raw_results_other_methods_2Tx_TEST.csv\"\n",
    "raw_df.to_csv(raw_out_csv, index=False)\n",
    "print(f\"\\nSaved RAW other-methods results: {raw_out_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877b397-6da9-40b2-aea0-54b7ec173083",
   "metadata": {},
   "source": [
    "# Save Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e3f4c3-656d-45ec-9b64-9f1279253a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def to_vec3(x):\n",
    "    if isinstance(x, list): return [float(x[0]), float(x[1]), float(x[2])]\n",
    "    if isinstance(x, str):  return list(map(float, ast.literal_eval(x)))\n",
    "    raise ValueError(\"Unexpected 3D vector format\")\n",
    "\n",
    "def to_length(v, L=5.0, eps=1e-12):\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    n = np.linalg.norm(v)\n",
    "    if n < eps: return v.tolist()\n",
    "    return (v * (L / n)).tolist()\n",
    "\n",
    "def angle_between_vectors(v1, v2, eps=1e-12):\n",
    "    v1 = np.asarray(v1, dtype=float); v2 = np.asarray(v2, dtype=float)\n",
    "    n1 = np.linalg.norm(v1); n2 = np.linalg.norm(v2)\n",
    "    if n1 < eps or n2 < eps: return np.nan\n",
    "    cos_t = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
    "    return float(np.degrees(np.arccos(cos_t)))\n",
    "\n",
    "def angles_matrix_deg(P, T):\n",
    "    \"\"\"Return KxK matrix of angular errors (degrees) between rows of P and T.\"\"\"\n",
    "    K = P.shape[0]\n",
    "    M = np.zeros((K, K), dtype=float)\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            M[i, j] = angle_between_vectors(P[i], T[j])\n",
    "    return M\n",
    "\n",
    "# ============================================================\n",
    "# Load base data and rebuild features (same preprocessing you used to train)\n",
    "# ============================================================\n",
    "DF_PATH = \"size_estimation.csv\"\n",
    "CONFIG_PATH = \"dataset/config.csv\"\n",
    "SPLIT_CSV = \"train_test_split_2Tx.csv\"   # created earlier\n",
    "\n",
    "df = pd.read_csv(DF_PATH)\n",
    "# keep same method filter you trained with\n",
    "df = df[df[\"method\"] == \"KMeans centers\"].copy()\n",
    "\n",
    "df[\"est_center\"] = df[\"est_center\"].apply(to_vec3)\n",
    "df[\"est_size\"]   = df[\"est_size\"].astype(float)\n",
    "df[\"true_size\"]  = df[\"true_size\"].astype(float)\n",
    "df[\"tx_index\"]   = df[\"tx_index\"].astype(int)\n",
    "\n",
    "# Infer K as before\n",
    "counts_per_file = df.groupby(\"file_index\")[\"tx_index\"].nunique()\n",
    "K_detected = int(counts_per_file.mode().iloc[0])\n",
    "if \"K\" in globals():\n",
    "    assert K == K_detected, f\"Inconsistent K: model K={K}, data K={K_detected}\"\n",
    "else:\n",
    "    K = K_detected\n",
    "print(f\"K={K}\")\n",
    "\n",
    "# Build per-file feature rows (same layout used for both models): X = [s0,c0x,c0y,c0z, s1,c1x,...]\n",
    "rows = []\n",
    "for fid, s in (df.groupby(\"file_index\")):\n",
    "    s = s.sort_values(\"tx_index\")\n",
    "    if set(s[\"tx_index\"].unique()) != set(range(K)):  # keep only complete scenes\n",
    "        continue\n",
    "    feat = []\n",
    "    true_sizes = []\n",
    "    est_centers_raw = []\n",
    "    for k in range(K):\n",
    "        feat.append(float(s.loc[s[\"tx_index\"] == k, \"est_size\"].values[0]))\n",
    "        c = list(map(float, s.loc[s[\"tx_index\"] == k, \"est_center\"].values[0]))\n",
    "        feat.extend(c)\n",
    "        est_centers_raw.append(c)\n",
    "        true_sizes.append(float(s.loc[s[\"tx_index\"] == k, \"true_size\"].values[0]))\n",
    "    rows.append({\n",
    "        \"file_index\": int(fid),\n",
    "        \"X\": feat,\n",
    "        \"true_sizes\": true_sizes,\n",
    "        \"raw_centers\": est_centers_raw,  # not used in output, but handy for diagnostics\n",
    "    })\n",
    "pairs = pd.DataFrame(rows).sort_values(\"file_index\").reset_index(drop=True)\n",
    "\n",
    "# Load ground-truth centers (angle targets) from config and scale each to length 5.0 (same as angle model)\n",
    "cfg = pd.read_csv(CONFIG_PATH)\n",
    "if \"file_index\" not in cfg.columns:\n",
    "    cfg = cfg.reset_index().rename(columns={\"index\":\"file_index\"})\n",
    "\n",
    "truth_map = {}\n",
    "for _, r in cfg.iterrows():\n",
    "    centers = r[\"tx_centers\"]\n",
    "    centers = ast.literal_eval(centers) if isinstance(centers, str) else centers\n",
    "    centers = [list(map(float, c)) for c in centers]\n",
    "    centers = [to_length(c, 5.0) for c in centers]\n",
    "    truth_map[int(r[\"file_index\"])] = centers\n",
    "\n",
    "# Add true angle targets (3K) to pairs\n",
    "y_angle_true = []\n",
    "for fid in pairs[\"file_index\"]:\n",
    "    centers = truth_map[int(fid)]\n",
    "    if len(centers) != K:\n",
    "        raise ValueError(f\"File {fid}: expected {K} centers in config, got {len(centers)}\")\n",
    "    y_angle_true.append(np.array(centers, dtype=float).reshape(-1))\n",
    "pairs[\"y_angle_true\"] = y_angle_true\n",
    "\n",
    "# ============================================================\n",
    "# Reuse the SAME test split\n",
    "# ============================================================\n",
    "split_df = pd.read_csv(SPLIT_CSV)\n",
    "test_files = set(split_df.loc[split_df[\"split\"] == \"test\", \"file_index\"].astype(int))\n",
    "\n",
    "pairs_test = pairs[pairs[\"file_index\"].isin(test_files)].copy().reset_index(drop=True)\n",
    "print(f\"Test scenes: {len(pairs_test)}\")\n",
    "\n",
    "# ============================================================\n",
    "# Predict with ANGLE model on test set\n",
    "#   Required globals: model, scaler_X, y_mean, y_std, K\n",
    "# ============================================================\n",
    "X_angle_test = np.array(pairs_test[\"X\"].to_list(), dtype=float)      # (N, 4K)\n",
    "Y_angle_true_flat = np.array(pairs_test[\"y_angle_true\"].to_list(), dtype=float)  # (N, 3K)\n",
    "\n",
    "# scale inputs with the angle-model scaler\n",
    "X_angle_test_s = scaler_X.transform(X_angle_test)\n",
    "\n",
    "import torch\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    Y_angle_pred_s = model(torch.from_numpy(X_angle_test_s.astype(np.float32))).cpu().numpy()\n",
    "Y_angle_pred = Y_angle_pred_s * y_std + y_mean   # (N, 3K)\n",
    "\n",
    "# reshape to (N, K, 3)\n",
    "Y_angle_pred = Y_angle_pred.reshape(-1, K, 3)\n",
    "Y_angle_true = Y_angle_true_flat.reshape(-1, K, 3)\n",
    "\n",
    "# ============================================================\n",
    "# Predict with SIZE model on the SAME test set\n",
    "#   Required globals: size, scaler_X_size, y_mean_size, y_std_size\n",
    "# ============================================================\n",
    "X_size_test  = X_angle_test                                   # same features\n",
    "true_sizes   = np.array(pairs_test[\"true_sizes\"].to_list())   # (N, K)\n",
    "\n",
    "X_size_test_s = scaler_X_size.transform(X_size_test)\n",
    "size.eval()\n",
    "with torch.no_grad():\n",
    "    y_size_pred_s = size(torch.from_numpy(X_size_test_s.astype(np.float32))).cpu().numpy()\n",
    "y_size_pred = y_size_pred_s * y_std_size + y_mean_size         # (N, K)\n",
    "\n",
    "# ============================================================\n",
    "# Build rows with Hungarian matching (by angle) PER SCENE\n",
    "# ============================================================\n",
    "out_rows = []\n",
    "for i, fid in enumerate(pairs_test[\"file_index\"].tolist()):\n",
    "    pred_centers = Y_angle_pred[i]    # (K,3)\n",
    "    true_centers = Y_angle_true[i]    # (K,3)\n",
    "\n",
    "    # Cost = angle error in degrees\n",
    "    cost = angles_matrix_deg(pred_centers, true_centers)\n",
    "    row_ind, col_ind = linear_sum_assignment(cost)  # match each pred (row) to a true (col)\n",
    "\n",
    "    # angles per matched pair (in pred order)\n",
    "    angles_deg = [cost[r, c] for r, c in zip(row_ind, col_ind)]\n",
    "    mean_angle = float(np.nanmean(angles_deg)) if len(angles_deg) else np.nan\n",
    "\n",
    "    # centers_est from ANGLE model (pred order 0..K-1)\n",
    "    centers_est_list = pred_centers.tolist()\n",
    "    # centers_true (ground-truth for ANGLE model) in native order 0..K-1\n",
    "    centers_true_list = true_centers.tolist()\n",
    "\n",
    "    # SIZE estimates (pred order 0..K-1)\n",
    "    cluster_sizes_est = y_size_pred[i].tolist()\n",
    "    # True sizes, matched to the pred order via col_ind (so each pred_k compares to true[col_ind[k]])\n",
    "    true_sizes_matched = true_sizes[i][col_ind].tolist()\n",
    "\n",
    "    out_rows.append({\n",
    "        \"file_index\": int(fid),\n",
    "        \"mean_angle_deg\": float(mean_angle),\n",
    "        \"angles_deg\": angles_deg,  # list\n",
    "        \"centers_est\": centers_est_list,\n",
    "        \"centers_true\": centers_true_list,\n",
    "        \"match_row_ind\": row_ind.tolist(),\n",
    "        \"match_col_ind\": col_ind.tolist(),\n",
    "        \"cluster_sizes\": cluster_sizes_est,\n",
    "        \"true_cluster_sizes_matched\": true_sizes_matched,\n",
    "    })\n",
    "\n",
    "# ============================================================\n",
    "# Save CSV\n",
    "# ============================================================\n",
    "result_df = pd.DataFrame(out_rows)\n",
    "\n",
    "# Serialize list-like columns as JSON strings for a clean 1-row-per-scene CSV\n",
    "list_cols = [\"angles_deg\", \"centers_est\", \"centers_true\", \"match_row_ind\", \"match_col_ind\",\n",
    "             \"cluster_sizes\", \"true_cluster_sizes_matched\"]\n",
    "for c in list_cols:\n",
    "    result_df[c] = result_df[c].apply(lambda x: json.dumps(x))\n",
    "\n",
    "OUT_CSV = \"angle_size_eval_test_2Tx.csv\"\n",
    "result_df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved: {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca87bb2-3993-4ebc-ba3e-dff163e113c5",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71440a1-c156-498b-bd88-70e5625ef614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, torch\n",
    "\n",
    "# angle\n",
    "torch.save({\n",
    "    \"K\": K,\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"y_mean\": y_mean,          # np array ok\n",
    "    \"y_std\":  y_std,\n",
    "}, \"angle_model_2tx.pt\")\n",
    "joblib.dump(scaler_X, \"angle_scaler_2tx.joblib\")\n",
    "\n",
    "# size\n",
    "torch.save({\n",
    "    \"K\": K,\n",
    "    \"model_state\": size.state_dict(),\n",
    "    \"y_mean\": y_mean_size,\n",
    "    \"y_std\":  y_std_size,\n",
    "}, \"size_model_2tx.pt\")\n",
    "joblib.dump(scaler_X_size, \"size_scaler_2tx.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cdf6dc-dcee-4710-a1d2-2e47e2fbe230",
   "metadata": {},
   "source": [
    "# Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287ad08-e8dc-47d7-bbda-ad83fc4416c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, torch\n",
    "\n",
    "angle_state = torch.load(\"angle_model_2tx.pt\", map_location=\"cpu\", weights_only=True)\n",
    "scaler_X    = joblib.load(\"angle_scaler_2tx.joblib\")\n",
    "\n",
    "size_state  = torch.load(\"size_model_2tx.pt\", map_location=\"cpu\", weights_only=True)\n",
    "scaler_X_size = joblib.load(\"size_scaler_2tx.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
